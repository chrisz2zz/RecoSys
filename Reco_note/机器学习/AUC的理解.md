#### 混淆矩阵

混淆矩阵中有着Positive、Negative、True、False的概念，其意义如下：

- **称预测类别为1的为Positive（阳性），预测类别为0的为Negative（阴性）。**
- **预测正确的为True（真），预测错误的为False（伪）。**

对上述概念进行组合，就产生了如下的混淆矩阵：

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22-3.png" style="zoom:50%;" />

**然后**，由此引出True Positive Rate（真阳率）、False Positive（伪阳率）两个概念：
$$
TPRate = \frac{TP}{TP+FN}
$$

$$
FPRate = \frac{FP}{FP+TN}
$$

仔细看这两个公式，发现其实TPRate就是TP除以TP所在的列，FPRate就是FP除以FP所在的列，二者意义如下：

- **TPRate的意义是所有真实类别为1的样本中，预测类别为1的比例**
- **FPRate的意义是所有真实类别为0的样本中，预测类别为1的比例**

#### AUC 定义

按照定义，**AUC即ROC曲线下的面积**，而ROC曲线的横轴是FPRate，纵轴是TPRate，**当二者相等时，即y=x**，如下图:

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22-4.png" style="zoom:33%;" />

表示的意义是：**对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的。**

换句话说，**分类器对于正例和负例毫无区分能力**，和**抛硬币**没什么区别，一个抛硬币的分类器是我们能想象的最差的情况，因此一般来说我们认为AUC的最小值为0.5（当然也存在预测相反这种极端的情况，AUC小于0.5，这种情况相当于分类器**总是**把对的说成错的，错的认为是对的）

而我们希望分类器达到的效果是：**对于真实类别为1的样本，分类器预测为1的概率（即TPRate），要大于真实类别为0而预测类别为1的概率（即FPRate），即y＞x**，因此大部分的ROC曲线长成下面这个样子：

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22-5.png" style="zoom:33%;" />

**最理想的情况**下，即没有真实类别为1而错分为0的样本——TPRate一直为1，也没有真实类别为0而错分为1的样本——FPrate一直为0，AUC为1，这便是AUC的极大值

**AUC的优势**，AUC**同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价**。

#### AUC的排序特性

相对于ROC线下面积的解释，个人更喜欢排序能力的解释

> **例如0.7的AUC，其含义可以大概理解为：给定一个正样本和一个负样本，在70%的情况下，模型对正样本的打分高于对负样本的打分。可以看出在这个解释下，我们关心的只有正负样本之间的分数高低，而具体的分值则无关紧要**

#### AUC对均匀正负样本采样不敏感

正由于AUC对分值本身不敏感，故**常见的正负样本采样，并不会导致auc的变化**。比如在点击率预估中，处于计算资源的考虑，有时候会**对负样本做负采样，但由于采样完后并不影响正负样本的顺序分布**。即假设采样是随机的，**采样完成后，给定一条正样本，模型预测为score1，由于采样随机，则大于score1的负样本和小于score1的负样本的比例不会发生变化**。

**但如果采样不是均匀的**，比如采用word2vec的negative sample，其**负样本更偏向于从热门样本中采样，则会发现auc值发生剧烈变化**

#### **AUC值本身有何意义**

在实际业务中，常**常会发现点击率模型的auc要低于购买转化率模型的auc**。正如前文所提，**AUC代表模型预估样本之间的排序关系，即正负样本之间预测的gap越大，auc越大**

通常，点击行为的成本要低于购买行为，从业务上理解，点击率模型中正负样本的差别要小于购买率模型，**即购买转化模型的正样本通常更容易被预测准。**

#### **AUC值本身的理论上线**

假设我们拥有一个无比强大的模型，可以准确预测每一条样本的概率，那么该模型的AUC是否为1呢？现实常常很残酷，样本数据中本身就会存在大量的歧义样本，**即特征集合完全一致，但label却不同**。因此就算拥有如此强大的模型，也不能让AUC为1.

因此，当我们拿到样本数据时，第一步应该看看有多少样本是特征重复，但label不同，这部分的比率越大，代表其“必须犯的错误”越多。学术上称它们为Bayes Error Rate，也可以从不可优化的角度去理解。

我们花了大量精力做的特征工程，很大程度上在缓解这个问题。当增加一个特征时，观察下时候减少样本中的BER，可作为特征构建的一个参考指标。

#### **AUC与线上业务指标的宏观关系**

**AUC毕竟是线下离线评估指标，与线上真实业务指标有差别。差别越小则AUC的参考性越高**。比如上文提到的点击率模型和购买转化率模型，虽然购买转化率模型的AUC会高于点击率模型，**但往往都是点击率模型更容易做，线上效果更好。**

**购买决策比点击决策过程长、成本重，且用户购买决策受很多场外因素影响，比如预算不够、在别的平台找到更便宜的了、知乎上看了评测觉得不好等等原因，这部分信息无法收集到，导致最终样本包含的信息缺少较大，模型的离线AUC与线上业务指标差异变大。**

**样本数据包含的信息越接近线上，则离线指标与线上指标gap越小。而决策链路越长，信息丢失就越多，则更难做到线下线上一致**。

#### **AUC提升和业务指标不一致**

好在实际的工作中，常常是模型迭代的auc比较，即新模型比老模型auc高，代表新模型对正负样本的排序能力比老模型好。理论上，这个时候上线abtest，应该能看到ctr之类的线上指标增长。

实际上经常会发生不一致，首先，我们得排除一些低级错误： 

**1. 排除bug，线上线下模型predict的结果要符合预期。** 

**2. 谨防样本穿越。比如样本中有时间序类的特征，但train、test的数据切分没有考虑时间因子，则容易造成穿越。**