### 1. 前言

- 本文继续学习《机器学习实战》的内容，首先学习最基础的线性回归内容。
- 在之前的NG课程中，了解过基础的线性回归内容，具体见该博文：[线性回归](https://yearing1017.site/2019/07/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/)

- 主要学习简单的线性回归和局部加权线性回归，并通过预测鲍鱼年龄的实例进行实战演练。

### 2. 回归

- 回归的目的是**预测数值型的目标值**。最直接的办法是**依据输入写出一个目标值的计算公式**。
- 例如以下方程：

$$
H = aX + bY
$$

- 这就是回归方程，其中的a和b称为回归系数，**求回归系数的过程就是回归**。
- 一旦有了这些回归系数，再给定输入，做预测就非常容易了。具体的做法是用回归系数乘以输入值，再将结果全部加在一起，就得到了预测值。
- 说到回归，一般都是指线性回归（linear regression），所以本文里的回归和线性回归代表同一个意思。线性回归意味着可以**将输入项分别乘以一些常量，再将结果加起来得到输出**。

### 3. 回归的实现

#### 3.1 用线性回归找到最佳拟合直线

- 应该怎么从一大堆数据里求出回归方程呢？假定输入数据存放在矩阵X中，结果存放在向量y中：

$$
X=\left[\begin{array}{lll}{x_{11}} & {x_{12}} & {x_{13}} \\ {x_{21}} & {x_{22}} & {x_{23}} \\ {x_{31}} & {x_{32}} & {x_{33}}\end{array}\right], y=\left[\begin{array}{l}{y_{1}} \\ {y_{2}} \\ {y_{3}}\end{array}\right]
$$

- 而回归系数存放在向量w中：

$$
\omega=\left[\begin{array}{l}{\omega_{1}} \\ {\omega_{2}} \\ {\omega_{3}}\end{array}\right]
$$

- 那么对于给定的数据x1，即矩阵X的第一列数据，预测结果u1将会通过如下公式给出：

$$
u_{1}=\left[\begin{array}{l}{x_{11}} \\ {x_{21}} \\ {x_{31}}\end{array}\right]^{T} *\left[\begin{array}{l}{\omega_{1}} \\ {\omega_{2}} \\ {\omega_{3}}\end{array}\right]
$$

- 现在的问题是，有了数据矩阵X和对对应的结果向量y，怎么求得w参数？一个常用的方法就是找出使误差最小的w。这里的误差是指预测u值和真实y值之间的差值，使用该误差的简单累加将使得正差值和负差值相互抵消，所以我们采用**平方误差和**。

- 平方误差和如下所示：

$$
\sum_{i=1}^{m} (y_i - x_i^{T}w)^{2}
$$

- 将上述表达式矩阵化，表示为如下：

$$
(Y - Xw)^T(Y - Xw)
$$

- 现在需要明确的目的：找到w，使平方误差和最小。因为我们认为平方误差和越小，说明线性回归拟合效果越好。

- 将上式用矩阵表示的误差和对w求导：

$$
\begin{array}{l}{\frac{\partial(y-X \omega)^{T}(y-X \omega)}{\partial \omega}} \\ {=\frac{\partial\left(y^{T} y-y^{T} X \omega-\omega^{T} X^{T} y+\omega^{T} X^{T} X \omega\right)}{\partial \omega}} \\ {=\frac{\partial y^{T} y}{\partial \omega}-\frac{\partial y^{T} X \omega}{\partial \omega}-\frac{\partial \omega^{T} X^{T} y}{\partial \omega}+\frac{\partial \omega^{T} X^{T} X \omega}{\partial \omega}} \\ {=0-X^{T} \mathrm{y}-\frac{\partial\left(\omega^{T} X^{T} y\right)^{T}}{\partial \omega}+2 X^{T} X \omega} \\ {=0-X^{T} \mathrm{y}-\frac{\partial y^{T} X \omega}{\partial \omega}+2 X^{T} X \omega} \\ {=0-X^{T} \mathrm{y}-\frac{\partial \omega}{\partial \omega}+2 X^{T} X \omega} \\ {=0-X^{T} \mathrm{y}-X^{T} \mathrm{y}+2 X^{T} X \omega} \\ {=2 X^{T}(y-X \omega)}\end{array}
$$

- 令求导得到的式子为0，求得：$\widehat{\omega}=\left(X^{T} X\right)^{-1} X^{T} y$

- w上方的小标记代表这是当前可以估计出的**w的最优解**。从现有数据上估计出的w可能并不是数据中的真实w值，所以这里使用了一个"帽"符号来表示它仅是w的一个最佳估计。值得注意的是，上述公式中包含逆矩阵，也就是说，**这个方程只在逆矩阵存在的时候使用，也即是这个矩阵是一个方阵，并且其行列式不为0**。

#### 3.2 实战1

- 数据集下载地址：[下载](https://github.com/yearing1017/Machine-Learning/blob/master/Regression/ex0.txt)
- 数据集预览：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14-1.jpg)

- 第一列都为1.0，即x0。第二列为x1，即x轴数据。第三列为x2，即y轴数据。首先绘制下数据，看下数据分布。编写代码如下：

```python
import matplotlib.pyplot as plt
import numpy as np

def loadDataSet(fileName):
	numFeat = len(open(fileName).readline().split('\t')) - 1 # 读出每行数据有几列内容
	xArr = []; yArr = []
	fr = open(fileName)
	for line in fr.readlines():
		lineArr = []
		curLine = line.strip().split('\t')
		for i in range(numFeat):
			lineArr.append(float(curLine[i]))
		xArr.append(lineArr)							 # xArr保存的是x0和x1两列数据
		yArr.append(float(curLine[-1]))
	return xArr,yArr

def plotDataSet():
	xArr,yArr = loadDataSet('ex0.txt')   # 加载数据
	n = len(xArr)                        # 数据个数
	xcord = []; ycord = []               # 样本点
	for i in range(n):
		xcord.append(xArr[i][1]); ycord.append(yArr[i])    # 添加样本点，xcord取xArr中的第2列内容
	fig = plt.figure()
	ax = fig.add_subplot(111)
	ax.scatter(xcord, ycord, s = 20, c = 'blue',alpha = .5)                #绘制样本点
	plt.title('DataSet')
	plt.xlabel('X')
	plt.show()

if __name__ == '__main__':
	plotDataSet()

```

- 运行结果：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14-2.jpg)

- 通过可视化数据，我们可以看到数据的分布情况。接下来，让我们根据上文中推导的回归系数计算方法，求出回归系数向量，并根据回归系数向量绘制回归曲线，编写代码如下：

```python
import matplotlib.pyplot as plt
import numpy as np

def loadDataSet(fileName):
	numFeat = len(open(fileName).readline().split('\t')) - 1 # 读出每行数据有几列内容
	xArr = []; yArr = []
	fr = open(fileName)
	for line in fr.readlines():
		lineArr = []
		curLine = line.strip().split('\t')
		for i in range(numFeat):
			lineArr.append(float(curLine[i]))
		xArr.append(lineArr)					# xArr保存的是x0和x1两列数据
		yArr.append(float(curLine[-1]))
	return xArr,yArr
"""
    函数说明:按照已有公式计算回归系数w
    Parameters:
        xArr - x数据集
        yArr - y数据集
    Returns:
        ws - 回归系数
"""
def standRegres(xArr,yArr):
	xMat = np.mat(xArr); yMat = np.mat(yArr).T # 将得到的y向量先转置为列向量
	xTx = xMat.T * xMat
	if np.linalg.det(xTx) == 0.0:
		print("该矩阵不可求逆")
		return
	ws = xTx.I * (xMat.T*yMat)
	return ws



def plotDataSet():
	xArr,yArr = loadDataSet('ex0.txt')   # 加载数据
	ws = standRegres(xArr,yArr)
	xMat = np.mat(xArr)
	yMat = np.mat(yArr)
	xCopy = xMat.copy()                  # 深拷贝xMat矩阵
	xCopy.sort(0)                        # 对xCopy进行由小到大的排序
	yHat = xCopy * ws                    # 计算对应的y值
	fig = plt.figure()
	ax = fig.add_subplot(111)
	ax.plot(xCopy[:, 1],yHat,c = 'red')  # 绘制回归曲线
	# 下面语句绘制样本点，flatten函数按行降维即取得一维数组，.A[0]即取第一个元素
	ax.scatter(xMat[:,1].flatten().A[0], yMat.flatten().A[0], s = 20, c = 'blue',alpha = .5)
	plt.title('DataSet')
	plt.xlabel('X')
	plt.show()

if __name__ == '__main__':
	plotDataSet()

```

- 运行结果如下：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14-3.jpg)

- 如何判断拟合曲线的拟合效果？我们可以使用**corrcoef方法**，来比较预测值和真实值的相关性。编写代码如下：

```python
if __name__ == '__main__':
    xArr, yArr = loadDataSet('ex0.txt')                                 # 加载数据集
    ws = standRegres(xArr, yArr)                                        # 计算回归系数
    xMat = np.mat(xArr)                                                 # 创建xMat矩阵
    yMat = np.mat(yArr)                                                 # 创建yMat矩阵
    yHat = xMat * ws
    print(np.corrcoef(yHat.T, yMat))
```

- np.corrcoef函数是用来计算两个矩阵的相关系数，结果返回一个矩阵。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14-4.jpg)

- 返回矩阵的行数 == 列数 == 矩阵1行数*矩阵2行数。yMat矩阵是1行n列。yHat是n行2列，转置之后2行n列，所以，返回结果矩阵是2行2列。
- 第i行j列表示第i组数据与第j组数据的相关系数。例如上图，对角线分别是矩阵1和矩阵1，矩阵2和矩阵2的相关系数，第1行2列表示矩阵1和矩阵2的相关系数，第2行1列表示矩阵2和矩阵1的相关系数。

#### 3.3 局部加权线性回归

- 线性回归的一个问题是有可能出现**欠拟合现象**，因为它求的是具有小均方误差的无偏估计。显而易见，如果模型欠拟合将不能取得好的预测效果。所以**有些方法允许在估计中引入一 些偏差，从而降低预测的均方误差**。

- 其中的一个方法是**局部加权线性回归（Locally Weighted Linear Regression，LWLR）**。在该方法中，我们给待预测点附近的每个点赋予一定的权重。与kNN一样，这种算法每次预测均需要事先选取出对应的数据子集。该算法解得回归系数W的形式如下：

$$
\widehat{\omega}=\left(X^{T} W X\right)^{-1} X^{T} W y
$$

- 其中W是一个矩阵，这个公式跟我们上面推导的公式的区别就在于W，它用来给每个点赋予权重。

- LWLR使用"核"（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是**高斯核**，高斯核对应的权重如下：

$$
w(i, i)=\exp \left(\frac{\left|x^{(i)}-x\right|^{2}}{-2 k^{2}}\right)
$$

- 这样我们就可以根据上述公式，编写局部加权线性回归，我们通过改变k的值，可以调节回归效果，编写代码如下：

```python
# -*- coding:utf-8 -*-
from matplotlib.font_manager import FontProperties
import matplotlib.pyplot as plt
import numpy as np
"""
函数说明:加载数据
    Parameters:
        fileName - 文件名
    Returns:
        xArr - x数据集
        yArr - y数据集
"""
def loadDataSet(fileName):
    numFeat = len(open(fileName).readline().split('\t')) - 1
    xArr = []; yArr = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr =[]
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        xArr.append(lineArr)
        yArr.append(float(curLine[-1]))
    return xArr, yArr

"""
函数说明:绘制多条局部加权回归曲线
"""
def plotlwlrRegression():
    font = FontProperties(fname=r"c:\windows\fonts\simsun.ttc", size=14)
    xArr, yArr = loadDataSet('ex0.txt')                    #加载数据集
    yHat_1 = lwlrTest(xArr, xArr, yArr, 1.0)               #根据局部加权线性回归计算yHat，k=1.0
    yHat_2 = lwlrTest(xArr, xArr, yArr, 0.01)              #根据局部加权线性回归计算yHat
    yHat_3 = lwlrTest(xArr, xArr, yArr, 0.003)             #根据局部加权线性回归计算yHat
    xMat = np.mat(xArr)                                    #创建xMat矩阵
    yMat = np.mat(yArr)                                    #创建yMat矩阵
    srtInd = xMat[:, 1].argsort(0)                         #排序，返回索引值
    xSort = xMat[srtInd][:,0,:]
    fig, axs = plt.subplots(nrows=3, ncols=1,sharex=False, sharey=False, figsize=(10,8))                                        
    axs[0].plot(xSort[:, 1], yHat_1[srtInd], c = 'red')    #绘制回归曲线
    axs[1].plot(xSort[:, 1], yHat_2[srtInd], c = 'red')    #绘制回归曲线
    axs[2].plot(xSort[:, 1], yHat_3[srtInd], c = 'red')    #绘制回归曲线
    axs[0].scatter(xMat[:,1].flatten().A[0], yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5)                																			 #绘制样本点
    axs[1].scatter(xMat[:,1].flatten().A[0], yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5)                																			 #绘制样本点
    axs[2].scatter(xMat[:,1].flatten().A[0], yMat.flatten().A[0], s = 20, c = 'blue', alpha = .5)                																			 #绘制样本点
    #设置标题,x轴label,y轴label
    axs0_title_text = axs[0].set_title(u'局部加权回归曲线,k=1.0',FontProperties=font)
    axs1_title_text = axs[1].set_title(u'局部加权回归曲线,k=0.01',FontProperties=font)
    axs2_title_text = axs[2].set_title(u'局部加权回归曲线,k=0.003',FontProperties=font)
    plt.setp(axs0_title_text, size=8, weight='bold', color='red')  
    plt.setp(axs1_title_text, size=8, weight='bold', color='red')  
    plt.setp(axs2_title_text, size=8, weight='bold', color='red')  
    plt.xlabel('X')
    plt.show()
    
"""
函数说明:使用局部加权线性回归计算回归系数w
    Parameters:
        testPoint - 测试样本点
        xArr - x数据集
        yArr - y数据集
        k - 高斯核的k,自定义参数
    Returns:
        ws - 回归系数
"""
def lwlr(testPoint, xArr, yArr, k = 1.0):
    xMat = np.mat(xArr); yMat = np.mat(yArr).T
    m = np.shape(xMat)[0]
    weights = np.mat(np.eye((m)))                  #创建权重对角矩阵,默认参数对角线为1，其余为0
    for j in range(m):                             #遍历数据集计算每个样本的权重
        diffMat = testPoint - xMat[j, :]                                 
        weights[j, j] = np.exp(diffMat * diffMat.T/(-2.0 * k**2))
    xTx = xMat.T * (weights * xMat)                                        
    if np.linalg.det(xTx) == 0.0:
        print("矩阵为奇异矩阵,不能求逆")
        return
    ws = xTx.I * (xMat.T * (weights * yMat))       #计算回归系数
    return testPoint * ws
"""
函数说明:局部加权线性回归测试
    Parameters:
        testArr - 测试数据集
        xArr - x数据集
        yArr - y数据集
        k - 高斯核的k,自定义参数
    Returns:
        ws - 回归系数
"""
def lwlrTest(testArr, xArr, yArr, k=1.0):  
    m = np.shape(testArr)[0]                        #计算测试数据集大小
    yHat = np.zeros(m)    
    for i in range(m):                              #对每个样本点进行预测
        yHat[i] = lwlr(testArr[i],xArr,yArr,k)
    return yHat
if __name__ == '__main__':
    plotlwlrRegression()
```

- 运行结果如下：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14-5.jpg)

- 可以看到，**当k越小，拟合效果越好。但是当k过小，会出现过拟合的情况**，例如k等于0.003的时候。

### 4. 预测鲍鱼年龄

- 接下来，我们将回归用于真实数据。在abalone.txt文件中记录了鲍鱼的年龄，这个数据来自UCI数据集合的数据。鲍鱼年龄可以从鲍鱼壳的层数推算得到。

- 数据集下载：[地址](https://github.com/yearing1017/Machine-Learning/blob/master/Regression/abalone.txt)

- 数据集预览：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14-6.jpg)

- 可以看到，数据集是多维的，所以我们很难画出它的分布情况。每个维度数据的代表的含义没有给出，不过我们只要知道**最后一列的数据是y值**就可以了，**最后一列代表的是鲍鱼的真实年龄，前面几列的数据是一些鲍鱼的特征，例如鲍鱼壳的层数等**。我们不做数据清理，直接用上所有特征，测试下我们的局部加权回归。

- 新建abalone.py文件，添加rssError函数，用于评价最后回归结果。编写代码如下：

```python
# -*- coding:utf-8 -*-
from matplotlib.font_manager import FontProperties
import matplotlib.pyplot as plt
import numpy as np
"""
函数说明:加载数据
    Parameters:
        fileName - 文件名
    Returns:
        xArr - x数据集
        yArr - y数据集
"""
def loadDataSet(fileName):
    numFeat = len(open(fileName).readline().split('\t')) - 1
    xArr = []; yArr = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr =[]
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        xArr.append(lineArr)
        yArr.append(float(curLine[-1]))
    return xArr, yArr
  
"""
函数说明:使用局部加权线性回归计算回归系数w
    Parameters:
        testPoint - 测试样本点
        xArr - x数据集
        yArr - y数据集
        k - 高斯核的k,自定义参数
    Returns:
        ws - 回归系数
"""
def lwlr(testPoint, xArr, yArr, k = 1.0):
    xMat = np.mat(xArr); yMat = np.mat(yArr).T
    m = np.shape(xMat)[0]
    weights = np.mat(np.eye((m)))                            #创建权重对角矩阵
    for j in range(m):                                       #遍历数据集计算每个样本的权重
        diffMat = testPoint - xMat[j, :]                                 
        weights[j, j] = np.exp(diffMat * diffMat.T/(-2.0 * k**2))
    xTx = xMat.T * (weights * xMat)                                        
    if np.linalg.det(xTx) == 0.0:
        print("矩阵为奇异矩阵,不能求逆")
        return
    ws = xTx.I * (xMat.T * (weights * yMat))                 #计算回归系数
    return testPoint * ws
  
"""
函数说明:局部加权线性回归测试
    Parameters:
        testArr - 测试数据集
        xArr - x数据集
        yArr - y数据集
        k - 高斯核的k,自定义参数
    Returns:
        ws - 回归系数
"""
def lwlrTest(testArr, xArr, yArr, k=1.0):  
    m = np.shape(testArr)[0]                               #计算测试数据集大小
    yHat = np.zeros(m)    
    for i in range(m):                                     #对每个样本点进行预测
        yHat[i] = lwlr(testArr[i],xArr,yArr,k)
    return yHat
  
"""
函数说明:计算回归系数w
    Parameters:
        xArr - x数据集
        yArr - y数据集
    Returns:
        ws - 回归系数
"""
def standRegres(xArr,yArr):
    xMat = np.mat(xArr); yMat = np.mat(yArr).T
    xTx = xMat.T * xMat                            #根据文中推导的公示计算回归系数
    if np.linalg.det(xTx) == 0.0:
        print("矩阵为奇异矩阵,不能求逆")
        return
    ws = xTx.I * (xMat.T*yMat)
    return ws
  
"""
误差大小评价函数
    Parameters:
        yArr - 真实数据
        yHatArr - 预测数据
    Returns:
        误差大小
"""
def rssError(yArr, yHatArr):
    return ((yArr - yHatArr) **2).sum()
  

if __name__ == '__main__':
    abX, abY = loadDataSet('abalone.txt')
    print('训练集与测试集相同:局部加权线性回归,核k的大小对预测的影响:')
    yHat01 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 0.1)
    yHat1 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 1)
    yHat10 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 10)
    print('k=0.1时,误差大小为:',rssError(abY[0:99], yHat01.T))
    print('k=1  时,误差大小为:',rssError(abY[0:99], yHat1.T))
    print('k=10 时,误差大小为:',rssError(abY[0:99], yHat10.T))
    print('')
    print('训练集与测试集不同:局部加权线性回归,核k的大小是越小越好吗？更换数据集,测试结果如下:')
    yHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)
    yHat1 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)
    yHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)
    print('k=0.1时,误差大小为:',rssError(abY[100:199], yHat01.T))
    print('k=1  时,误差大小为:',rssError(abY[100:199], yHat1.T))
    print('k=10 时,误差大小为:',rssError(abY[100:199], yHat10.T))
    print('')
    print('训练集与测试集不同:简单的线性归回与k=1时的局部加权线性回归对比:')
    print('k=1时,误差大小为:', rssError(abY[100:199], yHat1.T))
    ws = standRegres(abX[0:99], abY[0:99])
    yHat = np.mat(abX[100:199]) * ws
    print('简单的线性回归误差大小:', rssError(abY[100:199], yHat.T.A))
```

- 运行结果：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/14-7.jpg)

- 可以看到，当k=0.1时，训练集误差小，但是应用于新的数据集之后，误差反而变大了。这就是经常说道的**过拟合现象**。我们训练的模型，我们要保证测试集准确率高，这样训练出的模型才可以应用于新的数据，也就是要加强模型的普适性。

- 当k=1时，局部加权线性回归和简单的线性回归得到的效果差不多。这也表明一点，必须在未知数据上比较效果才能选取到最佳模型。那么最佳的核大小是10吗？或许是，但如果想得到更好的效果，应该用10个不同的样本集做10次测试来比较结果。

### 5. 总结

- 本文主要学习了简单的线性回归和局部加权线性回归。
- 训练的模型要在测试集比较它们的效果，而不是在训练集上。
- 在局部加权线性回归中，过小的核可能导致过拟合现象，即训练集表现良好，测试集表现就渣渣了。