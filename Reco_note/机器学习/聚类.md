#### 无监督学习

相比于监督学习，非监督学习的输入数据没有标签信息，需要通过算法模型来挖掘数据内在的结构和模式；

**非监督学习主要包含两大类学习方法：数据聚类和特征变量关联**。其中，**聚类算法往往是通过多次迭代来找到数据的最优分割**， 而特征变量关联则是利用各种相关性分析方法来找到变量之间的关系；

**支持向量机、逻辑回归、决策树等经典的机器学习算法主要用于分类问题**， 即根据一些已给定类别的样本，训练某种分类器，使得它能够对类别未知的样本进行分类。

与分类问题不同，**聚类是在事先并不知道任何样本类别标签的情况下，通过数据之间的内在关系把样本划分为若干类别，使得同类别样本之间的相似度高，不同类别之间的样本相似度低**

#### K均值聚类

K均值聚类（KMeans Clustering）是最基础和最常用的聚类算法。它的基本思想是，**通过迭代方式寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的代价函数最小**。代价函数可以定义为**各个样本距离所属簇中心点的误差平方和：**
$$
J(c, \mu)=\sum_{i=1}^{M}\left\|x_{i}-\mu_{c_{i}}\right\|^{2}
$$
其中 $x_i$ 代表第 i 个样本，$c_i$ 是$x_i$所属于的簇，$μ_{ci}$代表簇对应的中心点，M是样本总数。

K均值聚类的核心目标是**将给定的数据集划分成K个簇，并给出每个数据对应的簇中心点**。算法的具体步骤描述如下：

- **数据预处理**，如归一化、离群点处理等。

- **随机选取K个簇中心**，记为 $\mu_{1}^{(0)}, \mu_{2}^{(0)}, \ldots, \mu_{K}^{(0)}$

- 定义**代价函数**：$J(c, \mu)=min\sum_{i=1}^{M}\left\|x_{i}-\mu_{c_{i}}\right\|^{2}$

- 令 $t=0,1,2,…$ 为迭代步数，**重复下面过程**直到 $J$ 收敛：
  - 对于每一个样本$x_i$，**将其分配到距离最近的簇**
    - $c_{i}^{(t)} \leftarrow \underset{k}{\operatorname{argmin}}\left\|x_{i}-\mu_{k}^{(t)}\right\|^{2}$
  - 对于每一个类簇k，**重新计算该类簇的中心**
    - $\mu_{k}^{(t+1)} \leftarrow \underset{\mu}{\operatorname{argmin}} \sum_{i: c_{i}^{(t)}=k}\left\|x_{i}-\mu\right\|^{2}$

K均值算法在迭代时，假设当前 $J$ 没有达到最小值，那么**首先固定簇中心** ${μ_k}$，调整每个样例 $x_i$ 所属的类别 $c_i$ 来让 $J$ 函数减少；**然后固定**${c_i}$，调整簇中心${μ_k}$ 使$J$减小。这两个过程交替循环，$J$单调递减：当$J$递减到最小值时，${μ_k}$和${c_i}$也同时收敛

**结束的条件：重新计算该类簇的中心与上一轮一致，没有变化**

#### K均值聚类的优缺点及调优

K均值算法有一些缺点，例如**受初值和离群点的影响每次的结果不稳定、结果通常不是全局最优而是局部最优解、无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）、不太适用于离散分类**等。

K均值聚类的优点也是很明显和突出的，主要体现在：**对于大数据集，K均值聚类算法相对是可伸缩和高效的**，它的计算复杂度是O(NKt)接近于线性，其中N是数据对象的数目，K是聚类的簇数，t是迭代的轮数。**尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求**

**K均值算法的调优**一般可以从以下几个角度出发

- **数据归一化和离群点处理**
  - K均值聚类本质上是一种**基于欧式距离度量的数据划分方法**，**均值和方差大的维度将对数据的聚类结果产生决定性的影响**，所以**未做归一化处理和统一单位的数据是无法直接参与运算和比较的**
  - **离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，因此使用K均值聚类算法通常需要对数据做预处理**

- **合理选择K值**
  - K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。实际上，我们希望能够找到一些可行的办法来弥补这一缺点，或者说找到K值的合理估计方法。但是，**K值的选择一般基于经验和多次实验结果**。例如采用**手肘法**，我们可以尝试不同的K值，并将不同K值所对应的损失函数画成折线，横轴为K的取值，纵轴为误差平方和所定义的损失函数
  - 例如手肘法中，**当K=3时，存在一个拐点**，就像人的肘部一样；当$K\in(1,3)$时，曲线急速下降；当$K>3$时，曲线趋于平稳。**手肘法认为拐点就是K的最佳值**。

#### K均值聚类的缺点及相应的改进模型

**K均值算法的主要缺点**如下

- 需要人工预先确定初始K值，且该值和真实的数据分布未必吻合
- K均值只能收敛到局部最优，效果受到初始值很大
- 易受到噪点的影响

- 样本点只能被划分到单一的类中

**K-means++算法**

K均值的改进算法中，对初始值选择的改进是很重要的一部分。而这类算法中，最具影响力的当属K-means++算法。

**原始K均值算法最开始随机选取数据集中 K个点**作为聚类中心，**而K-means++按照如下的思想选取K个聚类中心。假设已经选取了n个初始聚类中心（0<n<K），则在选取第n+1个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为第n+1个聚类中心。在选取第一个聚类中心（n=1）时同样通过随机的方法。**

可以说这也符合我们的直觉，**聚类中心当然是互相离得越远越好**。当选择完初始点后，K-means++后续的执行和经典K均值算法相同，这也是对初始值选择进行改进的方法等共同点

#### 层次聚类

假设有 n 个待聚类的样本，对于层次聚类算法，它的步骤是：

- 步骤一：（初始化）将每个样本都视为一个聚类；
- 步骤二：计算各个聚类之间的相似度；
- 步骤三：寻找最近的两个聚类，将他们归为一类；
- 步骤四：重复步骤二，步骤三；直到所有样本归为一类

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-4.png" style="zoom: 33%;" />

整个过程就是建立一棵树，在建立的过程中，**可以在步骤四设置所需分类的类别个数，作为迭代的终止条件**，毕竟都归为一类并不实际。

#### 层次聚类相似度的度量

**Single Linkage**

- 又叫做 `nearest-neighbor` ，就是**取两个类中距离最近的两个样本的距离作为这两个集合的距离**。这种计算方式容易造成一种叫做 `Chaining` 的效果，两个 cluster 明明从“大局”上离得比较远，但是由于其中个别的点距离比较近就被合并了，并且这样合并之后 Chaining 效应会进一步扩大，最后会得到比较松散的 cluster

**Complete Linkage**

- 这个则完全是 `Single Linkage` 的反面极端，**取两个集合中距离最远的两个点的距离作为两个集合的距离**。其效果也是刚好相反的，限制非常大。这两种相似度的定义方法的共同问题就是指考虑了某个有特点的数据，而没有考虑类内数据的整体特点

**Average Linkage** 

- 这种方法就是**把两个集合中的点两两的距离全部放在一起求均值，相对也能得到合适一点的结果**。有时异常点的存在会影响均值，平常人和富豪平均一下收入会被拉高是吧，因此**这种计算方法的一个变种就是取两两距离的中位数**。

