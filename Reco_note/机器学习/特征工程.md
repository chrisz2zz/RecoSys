#### 归一化

**为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性**。

- 例如，分析一个人的身高和体重对健康的影响，如果 使用米（m）和千克（kg）作为单位，那么身高特征会在1.6～1.8m的数值范围 内，体重特征会在50～100kg的范围内，分析出来的结果显然会倾向于数值差别比较大的体重特征。想要得到更为准确的结果，就需要进行特征归一化 （Normalization）处理，使各指标处于同一数值量级，以便进行分析

对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内。最常用的方法主要有以下两种：

- **最大最小值归一化**（Min-Max Scaling）。它对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放

$$
X_{\text {norm }}=\frac{X-X_{\min }}{X_{\max }-X_{\min }}
$$

​		其中X为原始数据，$X_{max}, X_{min}$ 分别为数据最大值和最小值。

​		**应用场景**：**数值比较集中**的情况，如果数据较为稳定，不存在极端的最大最小值

- **零均值归一化**（Z-Score Normalization）也称为**标准化**。它会**将原始数据映射到均值为 0、标准差为1的标砖正态分布上**。具体来说，假设原始特征的均值为μ、标准差为σ，公式定义为:

$$
z=\frac{x-\mu}{\sigma}
$$

​		**应用场景：如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。**

- **非线性归一化：**经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射

  - y = log10(x)，即以10为底的对数转换函数，并且**所有样本数据均要大于等于1**
    $$
    x_{norm} = \frac{log_{10}(x)}{log_{10}(x_{max})} 或直接等于 log_{10}(x)
    $$

  - **sigmoid**
    $$
    x_{n e w}=\frac{1}{1+e^{-x}}
    $$
    

  - **三角函数转换**
    $$
    x_{n e w}=\frac{2}{\pi} * \arctan (x)
    $$
    

为什么需要对数值型特征做归一化呢？**以随机梯度下降的实例来说明归一化的重要性**。假设有两种数值型特征，x 1 的取值范围为 [0, 10]，x 2 的取值 范围为[0, 3]，于是可以构造一个目标函数符合下图中的等值图

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/27-8.png" style="zoom:33%;" />

在学习速率相同的情况下，$x_1$ 的更新速度会大于 $x_2$ ，需要较多的迭代才能找到最优解。

**如果将 $x_1$ 和 $x_2$ 归一化到相同的数值区间后，优化目标的等值图会变成图 b 中的圆形，$x_1$ 和 $x_2$ 的更新速度变得更为一致，容易更快地通过梯度下降找到最优解**

在实际应用中，**通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型**。

但**对于决策树模型则并不适用**，以C4.5为例，**决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比跟特征 是否经过归一化是无关的，因为归一化并不会改变样本在特征x上的信息增益**

