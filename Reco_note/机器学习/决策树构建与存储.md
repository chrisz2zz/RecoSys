### 1. 本文学习内容

- 决策树的构建
- 使用决策树进行分类读取
- 决策树的存储和读取

### 2. 决策树的构建

#### 2.1 ID3算法

- 算法的核心是在决策树各个结点上对应信息增益准则选择特征，递归地构建决策树。步骤如下：
  - 从根结点(root node)开始，对结点计算所有可能的特征的信息增益
  - 选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子节点
  - 再对子结点递归地调用以上方法，构建决策树
  - 直到所有特征的信息增益均很小或没有特征可以选择为止
- 数据集

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6-1.jpg)

- 在之前的笔记中，已求得特征A3(有自己的房子)的信息增益值最大，所以选择特征A3作为根结点的特征。
- 它将训练集D划分为两个子集D1(A3取值为"是")和D2(A3取值为"否")。
- 由于D1只有同一类的样本点（只有一个分类的最终结果），所以它成为一个叶结点，结点的类标记为“是”。

- 对D2则需要从特征A1(年龄)，A2(有工作)和A4(信贷情况)中选择新的特征，计算各个特征的信息增益
- 根据计算，选择信息增益最大的特征A2(有工作)作为结点的特征
- 由于A2有两个可能取值，从这一结点引出两个子结点：一个对应"是"(有工作)的子结点，包含3个样本，它们属于同一类，所以这是一个叶结点，类标记为"是"；另一个是对应"否"(无工作)的子结点，包含6个样本，它们也属于同一类，所以这也是一个叶结点，类标记为"否"。
- 使用这两个特征构建的决策树如下：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6-2.jpg)

#### 2.2 编写代码构建决策树

- 构建思想

  - 基于最好的特征属性值划分数据集，于特征值可能多于两个，可能会存在两个以上分支的数据集划分。

  - 第一次划分，数据向下传递到树分支的下一个节点
  - 在这个节点，可以再划分数据，因此采用递归的方法处理数据集
  - 递归结束条件：遍历完所有划分数据集的属性，或者每个分支下的所有实例都有相同的分类
  - 如果所有实例具有相同的分类，则得到一个叶子节点或者终止块
  - 如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时返回出现次数最多的分类作为叶子

- 我们使用字典存储树，上个图显示的树结果如下：

```python
{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}
```

- majorityCnt算法：返回次数最多的分类名称

```python
"""
函数说明:统计classList中出现此处最多的元素(类标签) 
Parameters:
    classList - 类标签列表
Returns:
    sortedClassCount[0][0] - 出现此处最多的元素(类标签)
"""
import operator
def majorityCnt(classList):
    classCount = {}
    for vote in classList:                                #统计classList中每个元素出现的次数
        if vote not in classCount.keys():classCount[vote] = 0   
        classCount[vote] += 1
    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True)        																					#根据字典的值降序排序
    return sortedClassCount[0][0]                     #返回classList中出现次数最多的元素
 
```

- 创建树的代码：

```python
"""
函数说明:创建决策树
 
Parameters:
    dataSet - 训练数据集
    labels - 分类属性标签
    featLabels - 存储选择的最优特征标签
Returns:
    myTree - 决策树
"""
def createTree(dataSet, labels, featLabels):
    classList = [example[-1] for example in dataSet]          #取分类标签(是否放贷:yes or no)
    if classList.count(classList[0]) == len(classList):       #如果类别完全相同则停止继续划分
        return classList[0]
    if len(dataSet[0]) == 1 or len(labels) == 0:         #遍历完所有特征时返回出现次数最多的类标签
        return majorityCnt(classList)
    bestFeat = chooseBestFeatureToSplit(dataSet)               #选择最优特征
    bestFeatLabel = labels[bestFeat]                           #最优特征的标签
    featLabels.append(bestFeatLabel)
    myTree = {bestFeatLabel:{}}                                #根据最优特征的标签生成树
    del(labels[bestFeat])                                      #删除已经使用特征标签
    featValues = [example[bestFeat] for example in dataSet]    #得到训练集中所有最优特征的属性值
    uniqueVals = set(featValues)                               #去掉重复的属性值
    for value in uniqueVals:                                   #遍历特征，创建决策树。                       
        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), labels, featLabels)
    return myTree
```

- 运行结果

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6-3.jpg)

#### 2.3 利用决策树进行分类

- 使用决策树以及用于构造决策树的标签向量
- 比较测试数据与决策树上的值，递归进入叶子节点

- 代码如下：

```python
"""
函数说明:使用决策树分类
Parameters:
    inputTree - 已经生成的决策树
    featLabels - 存储选择的最优特征标签：['有自己的房子', '有工作']
    testVec - 测试数据列表，顺序对应最优特征标签
Returns:
    classLabel - 分类结果
"""
def classify(inputTree, featLabels, testVec):
	firstStr = next(iter(inputTree))                                #获取决策树结点
	# print("firstStr:"+firstStr)         
	secondDict = inputTree[firstStr]                                 #下一个字典
	# print(secondDict)										
	featIndex = featLabels.index(firstStr)                                               
	# print(featIndex)										
	for key in secondDict.keys():
		if testVec[featIndex] == key:
			if type(secondDict[key]).__name__ == 'dict':
				classLabel = classify(secondDict[key], featLabels, testVec)
			else: classLabel = secondDict[key]
	return classLabel



if __name__ == '__main__':
	dataSet,labels = createDataSet()
	featLabels = []
	myTree = createTree(dataSet,labels,featLabels)
	print(featLabels)
	testVec = [0,1]
	result = classify(myTree,featLabels,testVec)
	if result == 'yes':
		print('放贷')
	if result == 'no':
		print('不放贷')
	print(myTree)
```

- 对比运行结果来看：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6-4.jpg)

- 输出结果分析：
  - featLabels：['有自己的房子', '有工作']
  - myTree：{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}
  - 函数第一轮 firstStr：有自己的房子
  - 函数第一轮secondDict：{0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}
  - 函数第一轮featIndex：0
  - testVec是按照featLables顺序来测试的，比较测试的值与secondDict的key
  - 若相等，且还可再分为字典，则递归进行
  - 若不可再分为字典，则返回该key对应的值，即分类

#### 2.4 决策树的存储

- 构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间
- 然而用创建好的决策树解决分类问题，则可以很快完成
- 为了解决这个问题，需要使用Python模块pickle序列化对象。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。
- 假设我们已经得到决策树{'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}，使用pickle.dump存储决策树。

```python
# -*- coding: UTF-8 -*-
import pickle
"""
函数说明:存储决策树
Parameters:
    inputTree - 已经生成的决策树
    filename - 决策树的存储文件名
Returns:
    无
"""
def storeTree(inputTree, filename):
    with open(filename, 'wb') as fw:
        pickle.dump(inputTree, fw)

if __name__ == '__main__':
    myTree = {'有自己的房子': {0: {'有工作': {0: 'no', 1: 'yes'}}, 1: 'yes'}}
    storeTree(myTree, 'classifierStorage.txt')
```

- 运行代码，在该Python文件的相同目录下，会生成一个名为classifierStorage.txt的txt文件，这个文件二进制存储着我们的决策树。我们可以使用sublime txt打开看下存储结果。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/6-5.jpg)

- 这个是个二进制存储的文件，我们也无需看懂里面的内容，会存储，会用即可
- 使用pickle.load进行载入即可，编写代码如下：

```python
# -*- coding: UTF-8 -*-
import pickle

"""
函数说明:读取决策树
Parameters:
    filename - 决策树的存储文件名
Returns:
    pickle.load(fr) - 决策树字典
"""
def grabTree(filename):
    fr = open(filename, 'rb')
    return pickle.load(fr)

if __name__ == '__main__':
    myTree = grabTree('classifierStorage.txt')
    print(myTree)
```

