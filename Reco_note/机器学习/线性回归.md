### 1、单变量线性回归

​	在[损失函数与梯度下降算法](<https://yearing1017.site/2019/05/31/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/>)一文中已单变量线性回归的模型表示、损失函数以及梯度下降求解，这里不再重复介绍，只是简单将其拿出来复习一下。

#### 1-1、模型表示

$$
h_\theta(x) = \theta_0+\theta_1x
$$

#### 1-2、损失函数

$$
J(\theta_0,\theta_1)=\frac{1}{2m}\sum_1^m(h_\theta(x^{(i)}-y^{(i)})^2
$$

#### 1-3、梯度下降算法求解

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2-1.jpg)

### 2、多变量线性回归

​	多变量线性回归（Linear Regression with mutiple variable）其实就是自变量（特征，features）是多个。假设还是以预测房价为例，以前只有一个自变量（房子的大小，特征是Size），这里假设房价不仅受房子的大小（Size）还受到房间的数量（bedroom）和房子的楼层（floor）以及房子的年限（years）影响，这里就有四个自变量（特征）。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2-2.png)

这就是一个多变量线性回归问题。因此我们可以构建如下的线性回归模型：
$$
h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4
$$
当然有的时候特征变量也会有很多，比如10个、20个、50个等。为了方便以后的表述，这里给出数据集的一些符号:

- m表示数据集（训练集）中样本的数量
- n表示自变量（特征）的数量
- $x^i$表示训练集中第i个样本输入
- $x^i_j$表示训练集中第i个样本中第j个特征

对照单变量线性回归以及上面给出的例子，下面总结线性回归的模型一般表示、损失函数以及梯度下降求解。

#### 2-1.模型表示

$$
h_\theta(x) = \theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+...+\theta_nx_n
$$

​	这里$x_1$可以表示为房子的大小（特征）、$x_2$表示为为房间的数量、$x_3$表示为房子的位置等等。而相对应的$\theta_1$和$\theta_2$和$\theta_3$等等可以表示为对应特征的参数。$\theta_0$可以简单理解为房子的基础价。

一般为了方便编程，我们会采用矩阵的方式用来简化多变量线性回归模型：
$$
h_\theta(x)=[\theta_0 \theta_1\theta_2 ... \theta_n] \left[
                                         \begin{matrix}
                                           x_1\\
                                           x_2\\
                                           ...\\
                                           x_n
                                          \end{matrix}
                                          \right]
$$


#### 2-2.损失函数

​	同样的根据单变量线性回归，为了减小预测值和真实值之间的差距，我们的目标就是选择出使得模型的误差最小的模型参数，这里仍然使用误差平方和来表示，其损失函数如下：
$$
J(\theta)=\frac{1}{2m}\sum_1^m(h_\theta(x^{(i)})-y^{(i)})^2
$$

#### 2-3.梯度下降算法

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2-3.jpg)

### 3、特征缩放与学习效率

#### 3-1、特征缩放

从刚刚的房价预测数据我们取两个特征，一个是房子的大小（size），另一个是房间的数量（num of bedrooms），明显看出房子的大小的取值范围是0-2200，而房间的数量取值是1-5，两者的数值差距很大。我们在NG学习总结-（二）中已经画过特征参数和损失函数的等高线图，这里我们以特征参数$\theta_1 $(size) 和特征参数$\theta_2 $ (number of bedrooms)，简化等高线图如下（NG的课程里面这个size假设是0-2000不要受上面数据集的影响）：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2-4.png)

从上图可以看出如果不同特征之间的取值范围差距过大，在使用梯度下降算法的过程中会造成损失函数收敛到最小值的速度太慢。因此为了加速损失函数的收敛，我们会将不同的特征取值范围粗略的处理成范围相近的取值。

这里给出一种方法是$x_i =\frac{x_i}{max(x_i)} ​$，通过这种方式，得出的每个特征的取值的范围大约会在[-1,1]之间。比如刚刚的房价预测中房子的大小（size）和房间的数量（number of bedrooms），经过这种方式处理后，可以得出下面的等高线图：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2-5.png)

经过这种方式的特征缩放后，收敛的速度会快很多（没有明确定义所有的特征值一定要严格在[-1,1]之间，比如原来的数据集中某一项特征的取值范围是[0.5-2]之间这样也是可以的，各个特征的取值范围接近即可）。另外一种方式是标准化（mean normalization）这是一种使用比较多的特征缩放方法，其计算方式如下：
$$
x_i =\frac{x_i-\mu_i}{max(x_i)-min(x_i)} 
$$

这里的$\mu_i$是指各项特征的取值的平均值。通过这种方式，特征缩放后的取值一般是[-0.5,0.5]的范围内。

#### 3-2、学习速率

我们知道梯度下降算法的求解公式如下：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2-6.gif)

但是我们怎么确定在梯度下降算法的求解过程中一定是正确的呢？换一句话说每次的迭代参数都应该会使损失函数减小，那我们怎么确保这个过程是正常的，这个时候需要去检测。如果给定搞一个有效的学习速率，那么经过每一次迭代，特征参数都能降低损失函数的值，这个过程应该就是正确的，至少可以说每一次迭代损失函数有收敛到最小值的趋势。（当经过一次迭代后，如果损失函数的值减小了小于0.001，那么我们可以认为是收敛了。）

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2-7.png)