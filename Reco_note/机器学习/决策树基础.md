### 1.什么是决策树

- 决策树(decision tree)是一种基本的分类与回归方法。
- 如下图所示的流程图就是一个决策树，长方形代表判断模块(decision block)，椭圆形成代表终止模块(terminating block)，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支(branch)，它可以达到另一个判断模块或者终止模块。
- 我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点(node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类。

- 如下图所示：长方形和椭圆形都是结点。长方形的结点属于内部结点，椭圆形的结点属于叶结点，从结点引出的左右箭头就是有向边。而最上面的结点就是决策树的根结点(root node)。

  ![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-1.jpg)

- 我们可以把决策树看成一个if-then规则的集合，将决策树转换成if-then规则的过程是这样的：由决策树的根结点(root node)到叶结点(leaf node)的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。

### 2. 使用决策树的步骤

- 收集数据：可以使用任何方法。比如想构建一个相亲系统，我们可以从媒婆那里，或者通过采访相亲对象获取数据。根据他们考虑的因素和最终的选择结果，就可以得到一些供我们利用的数据了。
- 准备数据：收集完的数据，我们要进行整理，将这些所有收集的信息按照一定规则整理出来，并排版，方便我们进行后续处理。
- 分析数据：可以使用任何方法，决策树构造完成之后，我们可以检查决策树图形是否符合预期。
- 训练算法：这个过程也就是构造决策树，同样也可以说是决策树学习，就是构造一个决策树的数据结构。
- 测试算法：使用经验树计算错误率。当错误率达到了可接收范围，这个决策树就可以投放使用了。

### 3. 决策树的构建

决策树要如何构建呢？通常，这一过程可以概括为3个步骤：特征选择、决策树的生成和决策树的修剪。

#### 3.1 特征选择

- 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。

- 通常特征选择的标准是信息增益(information gain)或信息增益比，为了简单，本文使用信息增益作为选择特征的标准。

- 那么，什么是信息增益？在讲解信息增益之前，让我们看一组实例，贷款申请样本数据表。

  ![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-2.jpg)

- 希望通过所给的训练数据学习一个贷款申请的决策树，用于对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。

- 特征选择就是决定用哪个特征来划分特征空间。比如，我们通过上述数据表得到两个可能的决策树，分别由两个不同特征的根结点构成。

  ![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-3.jpg)

- 图(a)所示的根结点的特征是年龄，有3个取值，对应于不同的取值有不同的子结点。图(b)所示的根节点的特征是工作，有2个取值，对应于不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益就能够很好地表示这一直观的准则。
- 什么是信息增益呢？在划分数据集之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。

##### 3.1.1 香农熵

- 在可以评测哪个数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵(entropy)。

- 熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。如果待分类的事物可能划分在多个分类之中，则符号$x_i$的信息定义为 ：
$$
  \mathrm{l}\left(x_{i}\right)=-\log _{2} p\left(x_{i}\right)
  $$
  
- 其中p(xi)是选择该分类的概率。通过上式，我们可以得到所有类别的信息。为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值(数学期望)，通过下面的公式得到:
  $$
  \mathrm{H}=-\sum_{\mathrm{i}=1}^{n} \mathrm{p}\left(x_{i}\right) \log _{2} p\left(x_{i}\right)
  $$
  

- 当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。

- 什么叫由数据估计？比如有10个数据，一共有两个类别，A类和B类。其中有7个数据属于A类，则该A类的概率即为十分之七。其中有3个数据属于B类，则该B类的概率即为十分之三。浅显的解释就是，这概率是我们根据数据数出来的。

- 我们定义贷款申请样本数据表中的数据为训练数据集D，则训练数据集D的经验熵为H(D)，|D|表示其样本容量，及样本个数。设有K个类Ck, = 1,2,3,...,K,|Ck|为属于类Ck的样本个数，因此经验熵公式就可以写为:
  $$
  \mathrm{H}(\mathrm{D})=-\sum_{k=1}^{K} \frac{\left|c_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
  $$
   

- 根据此公式计算经验熵H(D)，分析贷款申请样本数据表中的数据。最终分类结果只有两类，即放贷和不放贷。根据表中的数据统计可知，在15个数据中，9个数据的结果为放贷，6个数据的结果为不放贷。所以数据集D的经验熵H(D)为:
  $$
  H(D)=-\frac{9}{15} \log _{2} \frac{9}{15}-\frac{6}{15} \log _{2} \frac{6}{15}=0.971
  $$

#### 3.1.2 编写代码计算经验熵

- 我们先对数据集进行标注：
  - 年龄：0代表青年，1代表中年，2代表老年；
  - 有工作：0代表否，1代表是；
  - 有自己的房子：0代表否，1代表是；
  - 信贷情况：0代表一般，1代表好，2代表非常好；
  - 类别(是否给贷款)：no代表否，yes代表是

```python
from math import log

"""
函数说明:创建测试数据集
Parameters:
    无
Returns:
    dataSet - 数据集
    labels - 分类属性

"""
def createDataSet():
	dataSet = [[0, 0, 0, 0, 'no'],         #数据集
            [0, 0, 0, 1, 'no'],
            [0, 1, 0, 1, 'yes'],
            [0, 1, 1, 0, 'yes'],
            [0, 0, 0, 0, 'no'],
            [1, 0, 0, 0, 'no'],
            [1, 0, 0, 1, 'no'],
            [1, 1, 1, 1, 'yes'],
            [1, 0, 1, 2, 'yes'],
            [1, 0, 1, 2, 'yes'],
            [2, 0, 1, 2, 'yes'],
            [2, 0, 1, 1, 'yes'],
            [2, 1, 0, 1, 'yes'],
            [2, 1, 0, 2, 'yes'],
            [2, 0, 0, 0, 'no']]
	labels = ['不放贷','放贷']			#分类标签
	return dataSet, labels 				#返回数据

"""
函数说明:计算给定数据集的经验熵(香农熵)
Parameters:
    dataSet - 数据集
Returns:
    shannonEnt - 经验熵(香农熵)
"""

def calcShannonEnt(dataSet):
	numEntires = len(dataSet)          #数据集的行数
	labelCounts = {}				   #保存每个标签出现的次数的字典
	for featVec in dataSet:
		currentLabel = featVec[-1]     #提取每个数据的最后一个值即标签
		if currentLabel not in labelCounts.keys():
			labelCounts[currentLabel] = 0	#统计次数
		labelCounts[currentLabel] +=1
	shannonEnt = 0.0                                 #经验熵(香农熵)
	for key in labelCounts:
		prob = float(labelCounts[key]) / numEntires  #选择该标签(Label)的概率
		shannonEnt -= prob * log(prob, 2)            #利用公式计算
	return shannonEnt

if __name__ == '__main__':
	dataSet,labels = createDataSet()
	print(dataSet)
	print(calcShannonEnt(dataSet))
```

- 代码运行结果如下图所示，代码是先打印训练数据集，然后打印计算的经验熵H(D)，程序计算的结果与我们统计计算的结果是一致的，程序没有问题。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/5-1.jpg)

#### 3.1.3 条件熵

- 如何选择特征，需要看信息增益。也就是说，信息增益是相对于特征而言的，信息增益越大，特征对最终的分类结果影响也就越大，我们就应该选择对最终分类结果影响最大的那个特征作为我们的分类特征。

- 条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)  $H(Y|X)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望：
  $$
  \begin{array}{l}{\mathrm{H}(\mathrm{Y} | \mathrm{X})=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)} \\ {p_{i}=P\left(X=x_{i}\right), i=1,2, \cdots, \mathrm{n}}\end{array}
  $$

- 当条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的条件熵称为条件经验熵。

- 条件熵 ：也可说定义为在以某特征划分数据集时，对应该划分条件下的熵。

- 特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差：
  $$
  g(D, A)=H(D)-H(D | A)
  $$

- 信息增益是熵的减少或者是数据无序度的减少。

#### 3.1.4 编写代码计算信息增益

- 编写splitDataSet函数：安照某给定特征划分数据集，返回划分之后的数据集。

```python
"""
函数说明:按照给定特征划分数据集
Parameters:
    dataSet - 待划分的数据集
    axis - 划分数据集的特征
    value - 需要返回的特征的值
Returns:
    retDataSet
"""
def splitDataSet(dataSet, axis, value):       
    retDataSet = []                                     #创建返回的数据集列表
    for featVec in dataSet:                             #遍历数据集
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]             #去掉axis特征
            reducedFeatVec.extend(featVec[axis+1:])     #将符合条件的添加到返回的数据集
            retDataSet.append(reducedFeatVec)
    return retDataSet                                   #返回划分后的数据集
```

- 运行splitDataSet(dataSet, 0,1)，该函数为以第0个特征（特征值为1）划分数据。函数运行结果：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/5-2.jpg)

- extend和append的区别：

```python
a = [1,2,3]
b = [4,5,6]
a.append(b)
print(a)
#[1,2,3,[4,5,6]]

a = [1,2,3]
a.extend(b)
print(a)
#[1,2,3,4,5,6]
```

- 接下来是遍历数据集，以各个特征值划分数据集，然后计算熵，计算信息增益，找到最好的划分方式。

```python
"""
函数说明:选择最优特征 
Parameters:
    dataSet - 数据集
Returns:
    bestFeature - 信息增益最大的(最优)特征的索引值
"""
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) - 1                   #特征数量
    baseEntropy = calcShannonEnt(dataSet)               #计算数据集的香农熵
    bestInfoGain = 0.0                                  #信息增益
    bestFeature = -1                                    #最优特征的索引值
    for i in range(numFeatures):                        #遍历所有特征
        #获取dataSet的第i个所有特征
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)                       #创建set集合{},元素不可重复
        newEntropy = 0.0                                 #经验条件熵
        for value in uniqueVals:                         #计算信息增益
            subDataSet = splitDataSet(dataSet, i, value)       #subDataSet划分后的子集
            prob = len(subDataSet) / float(len(dataSet))       #计算子集的概率
            newEntropy += prob * calcShannonEnt(subDataSet)    #根据公式计算经验条件熵
        infoGain = baseEntropy - newEntropy                    #信息增益（熵的减少）
        print("第%d个特征的增益为%.3f" % (i, infoGain))           #打印每个特征的信息增益
        if (infoGain > bestInfoGain):                          #计算信息增益
            bestInfoGain = infoGain                            #更新信息增益，找到最大的信息增益
            bestFeature = i                                    #记录信息增益最大的特征的索引值
    return bestFeature                                         #返回信息增益最大的特征的索引值
```

- 划分给出的数据集，得到结果如下：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/5-3.jpg)

### 4. 总结

- 本文主要学习了：
  - 决策树
  - 特征选择、经验熵、条件经验熵、信息增益
  - 选择最优特征划分数据集