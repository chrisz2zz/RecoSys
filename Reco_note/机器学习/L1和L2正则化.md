#### 过拟合

过拟合就是所谓的模型对可见的数据过度自信, 非常完美的拟合上了这些数据, 如果具备过拟合的能力, 那么这个方程就可能是一个比较复杂的非线性方程 , 正是因为这里的 $x^3$ 和 $x^2$ 使得这条虚线能够被弯来弯去, 所以整个模型就会特别努力地去学习作用在 $x^3$ 和 $x^2$ 上的 $c 、d$ 参数. 

但是我们期望模型要学到的却是这条蓝色的曲线. 因为它能更有效地概括数据.而且只需要一个 $y=a+bx$ 就能表达出数据的规律. 或者是说, 蓝色的线最开始时, 和红色线同样也有 $c、d$ 两个参数, 可是最终学出来时, c 和 d 都学成了0, 虽然蓝色方程的误差要比红色大, 但是概括起数据来还是蓝色好. 那我们如何保证能学出来这样的参数呢? 这就是正则化出现的原因啦

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-5.jpg" style="zoom: 33%;" />

#### L1和L2正则化

**正则化，就是在原来的loss function的基础上，加上了正则化项或者称为模型复杂度惩罚项**。现在我们以最熟悉的线性回归为例子：

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-6.jpg" style="zoom: 33%;" />

**L1 公式**：$J = loss + \sum\lambda ||w||$

**L2 公式**：$J = loss + \sum\lambda ||w||_2^2$   符号：$|w|$表示向量的模，外面再加一层||代表绝对值，没有特别含义；$|w|_2$下面的2代表开根号

对于图1的线条, 我们一般用这个方程来求得模型 $y(x)$ 和 真实数据 $y$ 的误差, 而 $L1$ $L2$ 就只是在这个误差公式后面多加了一个东西, 让误差不仅仅取决于拟合数据拟合的好坏, 而且取决于像刚刚 $c$ $d$ 那些参数的值的大小。**如果是每个参数的平方, 那么我们称它为 L2正则化（岭回归）, 如果是每个参数的绝对值, 我们称为 L1 正则化（lasso回归）**

**为了控制这种正规化的强度, 我们会加上一个参数 $\lambda$, 并且通过 交叉验证 cross validation 来选择比较好的 $\lambda$**

#### 主要思想

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-7.jpg" style="zoom:33%;" />

拿 L2正则化来探讨一下, **机器学习的过程是一个通过修改参数 $\theta$ 来减小误差的过程, 可是在减小误差的时候非线性越强的参数, 比如在 $x^3$ 旁边的 $\theta_4$ 就会被修改得越多, 因为如果使用非线性强的参数就能使方程更加曲折, 也就能更好的拟合上那些分布的数据点**。 

$\theta_4$  说, 瞧我本事多大, 就让我来改变模型, 来拟合所有的数据吧, 可是它这种态度招到了误差方程的强烈反击, 误差方程就说: no no no no, 我们是一个团队, 虽然你厉害, 但也不能仅仅靠你一个人, 万一你错了, 我们整个团队的效率就突然降低了, 我得 hold 住那些在 team 里独出风头的人. 这就是整套正规化算法的核心思想

#### 结构风险最小化角度

**结构风险最小化： 在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度**

加了L1正则化和L2正则化之后，目标函数求解的时候，最终解有什么变化

**图像解释**（假设$x$为一个二维样本，那么要求解参数 $w$ 也是二维)：即 $x_1,x_2,w_1,w_2$

原函数（即没加正则化项的损失函数，例如RMSE）曲线等高线（**同颜色曲线上$w_1,w_2$的值都相同**）

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-8.jpg" style="zoom:25%;" />

**加入L1和L2正则化项后的函数图像**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-9.jpg" style="zoom:50%;" />

从上边两幅图中我们可以看出：

- **如果不加L1和L2正则化，对于线性回归这种目标函数凸函数的话，最终的优化的结果就是最里边的紫色的小圈圈等高线上的点**

- **当加入L1正则化的时候**，我们先画出 $|w_1| + |w_2| = F$ 的图像，也就是一个菱形，代表这些曲线上的点算出来的 1范数 $|w_1| + |w_2|$ 都为F。那我们**现在的目标是不仅是原曲线算得值要小（越来越接近中心的紫色圈圈），还要使得这个菱形越小越好（F越小越好)**。那么还和原来一样的话，过中心紫色圈圈的那个菱形明显很大，因此我们**要取到一个恰好的值。那么如何求值**呢？

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-10.jpg" style="zoom:25%;" />

**以同一条原曲线目标等高线来说，现在以最外圈的红色等高线为例**，我们看到，对于红色曲线上的每个点都可以做一个菱形，根据上图可知，**当这个菱形与某条等高线相切（仅有一个交点）的时候，这个菱形最小**，上图相对比较大的两个菱形对应的1范数更大。

用公式说这个时候能使得在相同的 **原损失函数loss** 下，由于相切的时候的 $\lambda ||w||$ 小，即 $|w_1| + |w_2|$  小，所以能够使得 $J = loss + \sum\lambda ||w||$ 更小

最终加入L1范数得到的解，一定是某个菱形和某条原函数等高线的切点。**我们经过观察可以看到，几乎对于很多原函数等高曲线，和某个菱形相交的时候及其容易相交在坐标轴（比如上图），也就是说最终的结果，解的某些维度及其容易是0，比如上图最终解是** $w=(0,x)$ **，这也就是我们所说的L1更容易得到稀疏解（解向量中0比较多)的原因**

当加入L2正则化的时候，分析和L1正则化是类似的，也就是说我们仅仅是从菱形变成了圆形而已，同样还是求原曲线和圆形的切点作为最终解。当然与L1范数比，我们这样求的L2范数的**从图上来看，不容易交在坐标轴上，但是仍然比较靠近坐标轴**。**因此这也就是我们老说的，L2范数能让解比较小（靠近0），但是比较平滑（不等于0）**

**综上所述，我们可以看见，加入正则化项，在最小化经验误差的情况下，可以让我们选择解更简单（趋向于0）的解。**

结构风险最小化： 在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。

**因此，加正则化项就是结构风险最小化的一种实现。**

#### 求导说明

光看着图说，L1的菱形更容易和等高线相交在坐标轴，一点都没说服力，只是个感性的认识，不过不要紧，其实是很严谨的，我们直接用
求导来证明，具体的证明这里有一个很好的答案了，简而言之就是假设现在我们是一维的情况下 $h(w) = f(w) + C|w|$ ，其中 $h(w)$ 是目标函数， $f(w)$ 是没加L1正则化项前的目标函数， $C|w|$ 是L1正则项，那么**要使得0点成为最值可能的点，虽然在0点不可导，但是我们只需要让0点左右的导数异号**，

- 即 $h_{\text {左 }}^{\prime}(0) * h_{\text {右 }}^{\prime}(0)=\left(f^{\prime}(0)+C\right) \quad\left(f^{\prime}(0)-C\right)<0$ 即可也就是 $C>f^{\prime}(0)$ 的情况下，0点都是可能的最值点

**两种 regularization 能不能把最优的 w 变成 0，取决于原先的损失函数在 0 点处的导数。**

- **如果本来导数不为 0，那么施加 L2 regularization 后导数依然不为 0，最优的 x 也不会变成 0。**
- 而**施加 L1 regularization 时**，**只要 regularization 项的系数 C 大于原先损失函数在 0 点处的导数的绝对值，w = 0 就会变成一个极小值点。**上面只分析了一个参数 w。**事实上 L1 regularization 会使得许多参数的最优值变成 0，这样模型就稀疏了。**

