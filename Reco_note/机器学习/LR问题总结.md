#### 1. 概括

逻辑回归**假设数据服从伯努利分布**，通过**极大化似然函数**的方法，运用**梯度下降来求解参数**，来达到**将数据二分类**的目的。

#### 2. 伯努利分布

> 伯努利分布：是一个**离散型概率分布**，若成功，则随机变量取值1；若失败，随机变量取值为0。成功概率记为p，失败为q = 1-p

$$
f_{X}(x)=p^{x}(1-p)^{1-x}=\left\{\begin{array}{ll}p & \text { if } x=1 \\ q & \text { if } x=0\end{array}\right.
$$

在逻辑回归中，既然假设了数据分布服从伯努利分布，那就存在一个成功和失败，对应二分类问题就是正类和负类，那么就应该有一个样本为正类的概率 p，和样本为负类的概率 q = 1-p
$$
\begin{array}{c}p=h_{\theta}(x ; \theta) \\ q=1-h_{\theta}(x ; \theta)\end{array}
$$
**模型参数** $\theta$ 和样本**特征** $x$ ( $x$ 可以是一个向量)

**逻辑回归的第二个假设是正类的概率由sigmoid的函数计算**，即：
$$
p=\frac{1}{1+e^{-\theta^{T} x}}
$$
预测样本为正类的概率：$p(y=1 \mid x ; \theta)=h_{\theta}(x ; \theta)=\frac{1}{1+e^{-\theta^{T} x}}$

预测样本为负类的概率：$p(y=0 \mid x ; \theta)=1 - h_{\theta}(x ; \theta)=\frac{1}{1+e^{\theta^{T} x}}$

写在一起，即预测样本的类别：
$$
\hat{y}=p=p(y=1 \mid x ; \theta)^{y}(1-p(y=1 \mid x ; \theta))^{1-y}
$$
$\hat{y}$是个概率，还没有到它真正能成为预测标签的地步，更具体的过程应该是分别求出正类的概率即时，和负类的概率时，比较哪个大，因为两个加起来是1，所以我们通常**默认的是只用求正类概率，只要大于0.5即可归为正类，但这个0.5是人为规定的，如果愿意的话，可以规定为大于0.6才是正类，这样的话就算求出来正类概率是0.55，那也不能预测为正类，应该预测为负类**

#### 3. 损失函数：交叉熵损失；或极大似然

逻辑回归的损失函数是它的极大似然函数

> 极大似然估计：利用已知的样本结果信息，反推最可能（最大概率）导致这些样本结果出现的模型参数值（模型已定，参数未知）

联系到逻辑回归里，一步步来分解上面这句话，首先确定一下模型是否已定，模型就是用来预测的那个公式：
$$
\hat{y}=p=p(y=1 \mid x ; \theta)^{y}(1-p(y=1 \mid x ; \theta))^{1-y}
$$
**模型参数** $\theta$ 和样本**特征** $x$ ，$y$ 是**标签**；

已知信息就是**在特征取这些值的情况下，它应该属于y类（正或负）反推最具有可能（最大概率）导致这些样本结果出现的参数**

>  举个例子，我们已经知道了一个样本点，是正类，那么我们把它丢入这个模型后，它预测的结果一定得是正类啊，正类才是正确的，才是我们所期望的，我们要**尽可能的让它最大**，这样才符合我们的真实标签。反过来一样的，如果你丢的是负类，那这个式子计算的就是负类的概率，同样我们要让它最大，所以此时不用区分正负类。
>
> 一个样本，不分正负类，丢入模型，多的不说，就是一个字，让它大

前面一直只提的是一个样本，但**对于整个训练集，我们当然是期望所有样本的概率都达到最大，也就是我们的目标函数**，**本身是个联合概率，但是假设每个样本独立，那所有样本的概率就可以写成如下，也就是似然函数**
$$
\prod_{i=1}^{N}p(y=1 \mid x_i ; \theta)^{y_i}(1-p(y=1 \mid x_i ; \theta))^{1-y_i}
$$
我们的目标是**最大化上面那个目标函数**，那我们就要向目标方向前进，要最大，那就求导啊，要求导，那就化简啊，不然太复杂了，那怎么化简呢?

- 第一步，**取对数，去掉连乘，变为连加**，直接给出化简后的结果；下面的形式也是**对数似然函数**

$$
loss= - \frac{1}{N} [\sum_{i=1}^{N} {y_i} log p(y=1 \mid x_i ; \theta)+ ({1-y_i})log(1-p(y=1 \mid x_i ; \theta))]
$$

- 第二步，**加负号，最大化变为最小化**，损失函数变为真实值和预测值的误差；下面即**极大似然与最小化交叉熵损失的转换**

$$
loss =\arg \max _{\theta} \sum_{i=1}^{N} y_{i} \log \hat{y}_{i}+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right) \\ =\arg \min _{\theta}-\sum_{i=1}^{N} y_{i} \log \hat{y}_{i}+\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right) \\ =\arg \min _{\theta} \sum_{i=1}^{N} H\left(y_{i}, \hat{y}_{i}\right)
$$

#### 4. 逻辑回归为什么用极大似然函数作为损失函数

**LR的损失函数及求导**
$$
loss= - \frac{1}{N} [\sum_{i=1}^{N} {y_i} log f_w(x^i)+ ({1-y_i})log(1-f_w(x^i))]
$$

$$
\frac{\partial}{\partial w_{j}} J(w)=\frac{1}{m} \sum_{i=1}^{m}\left(f_{w}\left(x^{i}\right)-y^{i}\right) x_{j}^{i}
$$

**MSE及求导**
$$
C=\frac{(y-\hat{y})^{2}}{2} \\ \frac{\partial C}{\partial w}=(\hat{y}-y) \sigma^{\prime}(z)(x)
$$
使用平方损失函数，会发现**梯度更新的速度和sigmod函数本身的梯度是很相关的**。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。**使用交叉熵的话就不会出现这样的情况，它的导数就是一个差值，误差大的话更新的就快，误差小的话就更新的慢点，这正是我们想要的**。

一般和平方损失函数（最小二乘法）拿来比较，因为线性回归用的就是平方损失函数，原因就是平方损失函数加上sigmoid的函数将会是一个非凸的函数，不易求解，会得到局部解，**用对数似然函数得到高阶连续可导凸函数，可以得到最优解。**

其次，是**因为对数损失函数更新起来很快，因为只和x，y有关，和sigmoid本身的梯度无关。**

#### 5. 优缺点

**优点：**

- 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
- 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
- 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
- 资源占用小,尤其是内存。因为只需要存储各个维度的特征值。
- 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cut off，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。

**缺点：**

- 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
- 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
- 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
- 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

#### 6. 给你一些很稀疏的特征，用LR还是树模型

参考：**很稀疏的特征表明是高维稀疏，用树模型（GBDT）容易过拟合。建议使用加正则化的LR**。

- 假设有1w 个样本， y类别0和1，100维特征，其中10个样本都是类别1，而特征 f1的值为0，1，且刚好这10个样本的 f1特征值都为1，其余9990样本都为0(在高维稀疏的情况下这种情况很常见)，我们都知道这种情况在树模型的时候，很容易优化出含一个使用 f1为分裂节点的树直接将数据划分的很好，但是当测试的时候，却会发现效果很差，因为这个特征只是刚好偶然间跟 y拟合到了这个规律，这也是我们常说的过拟合。
- 为什么线性模型就能对这种 case 处理的好？照理说 线性模型在优化之后不也会产生这样一个式子：y = W1*f1 + Wi*fi+….，其中 W1特别大以拟合这十个样本吗，因为反正 f1的值只有0和1，W1过大对其他9990样本不会有任何影响
  - 现在的模型普遍都会带着**正则项，而 lr 等线性模型的正则项是对权重的惩罚，也就是 W1一旦过大，惩罚就会很大，进一步压缩 W1的值，使他不至于过大**
  - 而**树模型则不一样，树模型的惩罚项通常为叶子节点数和深度等，而我们都知道，对于上面这种 case，树只需要一个节点就可以完美分割9990和10个样本，惩罚项极其之小**.
- 这也就是为什么在高维稀疏特征的时候，线性模型会比非线性模型好的原因了：**带正则化的线性模型比较不容易对稀疏特征过拟合**

