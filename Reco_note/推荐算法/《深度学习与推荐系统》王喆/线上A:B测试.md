#### 前言

- 本文记录了《深度学习推荐系统实战》**第26讲在线A/B测试**的要点
- 课程地址：[深度学习推荐系统实战](https://time.geekbang.org/column/intro/349)

#### 如何理解 A/B 测试

- A/B 测试又被称为“分流测试”或“分桶测试”，它**通过把被测对象随机分成 A、B 两组，分别对它们进行对照测试的方法得出实验结论。**
- 具体到推荐模型测试的场景下，它的流程是这样的：**先将用户随机分成实验组和对照组，然后给实验组的用户施以新模型，给对照组的用户施以旧模型，再经过一定时间的测试后，计算出实验组和对照组各项线上评估指标，来比较新旧模型的效果差异，最后挑选出效果更好的推荐模型**。

#### A/B测试的优势

- **首先，离线评估无法完全还原线上的工程环境**。 一般来讲，离线评估往往不考虑线上环境的延迟、数据丢失、标签数据缺失等情况，或者说很难还原线上环境的这些细节。因此，**离线评估环境只能说是理想状态下的工程环境，得出的评估结果存在一定的失真现象。**

- **其次，线上系统的某些商业指标在离线评估中无法计算**。 离线评估一般是针对模型本身进行评估的，无法直接获得与模型相关的其他指标，特别是商业指标。**离线评估关注的往往是 ROC 曲线、PR 曲线的改进，而线上评估却可以全面了解推荐模型带来的用户点击率、留存时长、PV 访问量这些指标的变化。其实，这些指标才是最重要的商业指标**，跟公司要达成的商业目标紧密相关，而它们都要由 A/B 测试进行更全面准确的评估。

- 最后是**离线评估无法完全消除数据有偏（Data Bias）现象的影响。** 什么叫“数据有偏”呢？因为离线数据都是系统利用当前算法生成的数据，因此这些数据本身就不是完全客观中立的，它是用户在当前模型下的反馈。所以说，用户本身有可能已经被当前的模型“带跑偏了”，你再用这些有偏的数据来衡量你的新模型，得到的结果就可能不客观。

#### A/B 测试的“分桶”和“分层”原则

- **A/B 测试的原理就是把用户分桶后进行对照测试**。

- 在 A/B 测试分桶的过程中，我们需要注意的是**样本的独立性和分桶过程的无偏性**。这里的**“独立性”指的是同一个用户在测试的全程只能被分到同一个桶中。“无偏性”指的是在分桶过程中用户被分到哪个实验桶中应该是一个纯随机的过程。**

- 举个简单的例子，我们把用户 ID 是奇数的用户分到对照组，把用户 ID 是偶数的用户分到实验组，这个策略只有在用户 ID 完全是随机生成的前提下才能说是无偏的，如果用户 ID 的奇偶分布不均，我们就无法保证分桶过程的无偏性。所以在实践的时候，我们经常会使用一些比较复杂的 Hash 函数，让用户 ID 尽量随机地映射到不同的桶中。
- 说完了分桶，那**什么是分层**呢？要知道，**在实际的 A/B 测试场景下，同一个网站或应用往往要同时进行多组不同类型的 A/B 测试。**比如，前端组正在进行不同 App 界面的 A/B 测试的时候，后端组也在进行不同中间件效率的 A/B 测试，同时算法组还在进行推荐场景 1 和推荐场景 2 的 A/B 测试。这个时候问题就来了，**这么多 A/B 测试同时进行，我们怎么才能让它们互相不干扰呢？**

- **A/B 测试的分层及层内分桶原则：层与层之间的流量“正交”，同层之间的流量“互斥”。**

- 层与层之间的流量“正交”，它指的是**层与层之间的独立实验的流量是正交的，一批实验用的流量穿越每层实验时，都会再次随机打散，然后再用于下一层的实验**。

- 假设，**在一个 X 层的实验中，流量被随机平均分为 X1（蓝色）和 X2（白色）两部分。当它们穿越到 Y 层的实验之后，X1 和 X2 的流量会被随机且均匀地分配给 Y 层的两个桶 Y1 和 Y2**。如果 Y1 和 Y2 的 X 层流量分配不均匀，那么 Y 层的样本就是有偏的，Y 层的实验结果就会被 X 层的实验影响，也就无法客观地反映 Y 层实验组和对照组变量的影响。

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/28-6.jpg" style="zoom: 33%;" />

- 同层之间的流量“互斥”。这里的“互斥”具体有 2 层含义：
  - **如果同层之间进行多组 A/B 测试，不同测试之间的流量不可以重叠，这是第一个“互斥”；**
  - **一组 A/B 测试中实验组和对照组的流量是不重叠的，这是第二个“互斥”。**

- 在基于用户的 A/B 测试中，“互斥”的含义可以被进一步解读为，**不同实验之间以及 A/B 测试的实验组和对照组之间的用户是不重叠的**。特别是对推荐系统来说，用户体验的一致性是非常重要的。也就是说我们不可以让同一个用户在不同的实验组之间来回“跳跃”，这样会严重损害用户的实际体验，也会让不同组的实验结果相互影响。因此**在 A/B 测试中，保证同一用户始终分配到同一个组是非常有必要的**。

- A/B 测试的“正交”与“互斥”原则共同保证了 A/B 测试指标的客观性，而且由于分层的存在，也让功能无关的 A/B 测试可以在不同的层上执行，充分利用了流量资源。

#### 线上 A/B 测试的评估指标

- 一般来说，A/B 测试是模型上线前的最后一道测试，通过 A/B 测试检验的模型会直接服务于线上用户，来完成公司的商业目标。因此，**A/B 测试的指标应该与线上业务的核心指标保持一致。这就需要我们因地制宜地制定最合适的推荐指标了。**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/28-7.jpg" style="zoom:33%;" />

- 线上 A/B 测试的指标和离线评估的指标（诸如 AUC、F1- score 等），它们之间的差异非常大。这主要是因为，离线评估不具备直接计算业务核心指标的条件，因此退而求其次，选择了偏向于技术评估的模型相关指标，但公司更关心的是能够驱动业务发展的核心指标，这也是 A/B 测试评估指标的选取原则。