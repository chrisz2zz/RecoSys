#### 注意力机制解决的问题

在计算能力有限的情况下，**「注意力机制（AttentionMechanism）」** 作为一种资源分配方案，将有限的计算资源用来处理更重要的信息，是解决信息超载问题的主要手段

从本质上理解，Attention是从大量信息中有筛选出少量重要信息，并聚焦到这些重要信息上，忽略大多不重要的信息。权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。

Attention机制的具体计算过程，对目前大多数方法进行抽象的话，可以将其归纳为两个过程：**第一个过程是根据Query和Key计算权重系数**，**第二个过程根据权重系数对Value进行加权求和**。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图展示的三个阶段。

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25-5.png" style="zoom:33%;" />

#### 简单的注意力机制

注意力机制的计算分为两步：

- **在所有的输入信息上计算注意力分布，即权重**
- **根据注意力分布来计算所有输入信息的加权平均**

**注意力分布的计算**

- 输入：用$\mathbf{X}=\left[x_{1}, \ldots, x_{N}\right] \in \mathbb{R}^{d \times N}$表示**N组输入向量**，其中每个向量的维度是d维

- 查询向量q：为了从N个输入的向量中选择出和某个特定任务相关的信息，引入一个和任务相关的表示，即**查询向量q**

- 计算方法：通过一个score函数来表示**每个输入向量和查询向量之间的相关性**，即**给定X和q，选择第i个输入向量的权重**$\alpha_n$

$$
\begin{aligned} \alpha_{n} &=\operatorname{softmax}\left(score\left(\mathbf{x}_{n}, \mathbf{q}\right)\right) \\ &=\frac{\exp \left(score\left(\mathbf{x}_{n}, \mathbf{q}\right)\right)}{\sum_{j=1}^{N} \exp \left(score\left(\mathbf{x}_{j}, \mathbf{q}\right)\right)} \end{aligned}
$$

$\alpha_n$ 称为**「注意力分布」**，也可以说是在给定任务相关的查询 q 时，**第 n 个输入向量受关注的程度**；score函数的主要计算方式如下：

- **内积**：$score(\mathbf{x},q) = q^{T} \mathbf{x}$
- **过一层全连接神经网络，增加非线性：**$score(\mathbf{x},q) = q^Ttanh(W^T\mathbf{x}+b)$
- **加性模型：**$score(\mathbf{x}, \mathbf{q})=\mathbf{v}^{T} \tanh (\mathbf{W} \mathbf{x}+\mathbf{U} \mathbf{q})$

- **缩放点积型：**$score(\mathbf{x}, \mathbf{q})=\frac{\mathbf{q}^{\mathbf{T}} \mathbf{x}}{\sqrt{d_{}}}$

- **双线性模型：**$score(\mathbf{x}, \mathbf{q})= q^TW\mathbf{x}$
  - 解释双线性：中间的 W 是一个矩阵，肯定可以分解为 $l^Th$，所以$q^TW\mathbf{x} = (q^Tl^T)(h\mathbf{x})$，相当于对 q 和 x 做了双线性变换

其中$\mathbf{v},\mathbf{W},\mathbf{U} $为可学习的参数，$D$ 为输入向量的维度

**「加权平均：」**

**得到注意力分布后，对输入信息汇总：**
$$
\operatorname{att}(\mathbf{X}, \mathbf{q})=\sum_{n=1}^{N} \alpha_{n} \mathbf{x}_{n}
$$
<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25-4.jpg" style="zoom:33%;" />

#### Self-attention 自注意力机制

**自注意力机制是注意力机制的变体，其减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性**

自注意力机制在文本中的应用，主要是通过计算单词间的互相影响，来解决长距离依赖问题

**计算过程：**

- 首先根据 N 个输入向量，分别乘上三个待学习的参数矩阵$W_q,W_k,W_v$ 计算得到$ q_i, k_i, v_i$
- 通过 $q_i, k_i$ 计算得到 注意力权重 $\alpha_i$

- **对权重施以softmax激活函数**；为了梯度的稳定，对权重进行归一化
- 对于每个输入 $x_i$ 当做 $q$，剩下的 $x_i$ 当做 $k$，先计算$\alpha_{i.j}$，**加权求和**最终的输出$Z_i$
  - $z_1 = \alpha_{1.1}v_1 + \alpha_{1.2}v_2 + ... + \alpha_{1.n}v_n$

**知乎上的一个例子图示上述步骤**

对于每一个输入向量a，在本例中也就是每一个词向量，经过self-attention之后都输出一个向量b，这个向量b是考虑了所有的输入向量才得到的，**这里有四个词向量a对应就会输出四个向量b**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25-6.png" style="zoom:33%;" />

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25-7.png" style="zoom:50%;" />

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25-8.png" style="zoom:50%;" />

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25-9.png" style="zoom:50%;" />

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/25-10.png" style="zoom:50%;" />

