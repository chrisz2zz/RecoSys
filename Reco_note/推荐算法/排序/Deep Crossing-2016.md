### 前言

- Deep Crossing模型是Bing搜索广告2016年公布的算法。[原文链接](https://link.zhihu.com/?target=http%3A//www.kdd.org/kdd2016/papers/files/adf0975-shanA.pdf)
- 当尝试去让一个机器学习的模型变的更好的时候，我们常常会手动给特征去做一些排列组合。这些特征排列组合理论上来说模型是可以自己理解出来的，但是常常由于模型大小，或者由于模型本身的特性，而造成拟合不了特征数据的分布。这个时候手动的去做一些特征的排列组合是可以让模型变得更好。在这个论文里，微软的人提出了**让深度学习的模型自己去做特征的排列组合**，他们叫这个模型Deep Crossing Model。

### 1. 模型应用场景

- Deep Crossing模型的应用场景是微软搜索引擎Bing中的搜索广告推荐场景。用户在搜索引擎中输入搜索词之后，搜索引擎除了会返回相关结果，还会返回与搜索词相关的广告。尽可能地增加搜索广告的点击率，准确地预测广告点击率，并以此作为广告排序的指标之一，是非常重要的工作，也是Deep Crossing模型的优化目标。
- 微软使用的特征如下表所示，这些特征可以分为三类：
  - **一类是可以被处理成one-hot或者multi-hot向量的类别型特征**，包括用户搜索词(query)、广告关键词(keyword)、广告标题(title)、落地页（landing page）、匹配类型(match type)；
  - **一类是数值型特征，微软称其为计数型特征**，包括点击率、预估点击率（click prediction）；
  - **一类是需要进一步处理的特征**，包括广告计划（campaign）、曝光计划(impression)、点击样例(click)等。

| 特征       | 特征含义                                                     |
| ---------- | ------------------------------------------------------------ |
| 搜索词     | 用户在搜索框中的输入                                         |
| 广告关键词 | 广告主为广告添加的描述产品的关键词                           |
| 广告标题   | 吸引用户的标题                                               |
| 落地页     | 点击广告后的落地页面                                         |
| 匹配类型   | 广告主选择的广告-搜索词匹配类型（精准匹配、短语匹配、语义匹配） |
| 点击率     | 广告的历史点击率                                             |
| 预估点击率 | 另一个CTR模型的CTR预估值                                     |
| 广告计划   | 广告主创建的广告投放计划，预算、定向条件等                   |
| 曝光样例   | 一个广告曝光的例子。记录了广告在实际曝光场景中的相关信息     |
| 点击样例   | 一个广告点击的例子，记录了广告在实际点击场景中的相关信息     |

### 2. 特征表示

- **类别型特征可以通过one-hot或multi-hot编码生成特征向量，数值型特征则可以直接拼接进特征向量中**，在生成所有输入特征的向量表达后，Deep Crossing模型利用该特征向量进行CTR预估。深度学习网络的特点是可以根据需求灵活地对网络结构进行调整，从而达成从原始特征向量到最终的优化目标的端到端的训练目的。

### 3. 模型结构

- 为完成端到端的训练，需要解决的问题：
  - 解决**稀疏特征向量稠密化**的问题
  - 解决**特征自动交叉组合**的问题
  - 在输出层中达成问题设定的优化目标
- 模型设置了不同的神经网络层来解决上述问题，Embedding, Stacking, The Residual Unit 跟 Scoring layer，模型层次如下图所示：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/8-1.png)

- **Embedding层：将稀疏的类别型特征向量转换为稠密的Embedding向量**。上图可看出，每个特征经过该层之后，会转换成Embedding向量。该层的结构以经典的全连接层结构为主，但现在已经成为了研究广泛的问题，衍生出了Word2vec、Graph Embedding等多种不同的方法。上图还可看出**，数值型的特征不需要经过Embedding层，直接进入了Stacking层**。

- **Stacking层：也可叫做堆叠层（连接层），把不同的Embedding特征和数值型特征拼到一起，形成新的含有全部特征的特征向量**
- **Multiple Residual Units层**：该层的主要结构是多层感知机，相比标准的以感知机为基本单元的神经网络，Deep Crossing模型采用了**多层残差网络**作为MLP的具体实现。在推荐模型中的应用，也是残差网络首次在图像识别领域之外的成功推广。通过多层残差网络对特征向量各个维度进行充分的交叉组合，使模型能够抓取到更多的非线性特征和组合特征的信息，进而使深度学习模型在表达能力上较传统机器学习模型大为增强。
- 残差层由如下图的残差单元构造而成。**Deep Crossing简单的修改了残差单元，不适用卷积核**。残差单元的独特之处在于两个，(1)它是将原输入特征通过两层以ReLU为激活函数的全连接层后，生成输出向量 。(2) 输入可以通过一个短路通路直接与输出向量进行元素加操作，生成最终的输出向量。在这样的结构下，残差单元中的两层ReLU网络其实拟合的是输出和输入之间的“残差”，这就是残差神经网络名称的由来。


![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/8-2.png)

- 残差神经网络的诞生主要解决了两个问题：
  - 神经网络是不是越深越好？对于传统的基于感知机的神经网络，当网络加深之后，往往存在过拟合现象，即网络越深，在测试集上的表现越差。而在残差神经网络中，可以越过两层ReLU网络，减少过拟合现象的发生。
  - 当神经网络足够深时，往往存在严重的梯度消失现象。梯度消失现象是指在梯度反向传播过程中，越靠近输入端，梯度的幅度越小，参数收敛的速度越慢。残差单元使用了ReLU激活函数取代原来的sigmoid激活函数。此外，输入向量短路相当于直接把梯度毫无变化地传递到下一层，这也使残差网络的收敛速度更快。

- **Scoring层**：Scoring层作为输出层，就是为了拟合优化目标而存在的。对于CTR预估这类二分类问题，Scoring层往往使用的是逻辑回归模型，而对于图像分类等多分类问题，Scoring层往往采用softmax模型。

- Deep Crossing模型中没有任何人工特征工程的参与，原始特征经Embedding后输入神经网络层，将全部特征交叉的任务交给模型。Deep Crossing模型可以通过调整神经网络的深度进行特征之间的“深度交叉”，这也是Deep Crossing名称的由来。

