#### 双塔模型

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/28-1.png" style="zoom:33%;" />

双塔模型结构非常简单，如上图所示，左侧是用户塔，右侧是Item塔，可将特征拆分为两大类：

- **用户相关特征**（用户基本信息、群体统计属性以及行为过的Item序列等）
- **Item相关特征**（Item基本信息、属性信息等）
- 原则上，**Context上下文特征可以放入用户侧塔**

对于这两个塔本身，则是经典的DNN模型，从特征OneHot到特征Embedding，再经过几层MLP隐层，两个塔分别输出用户Embedding和Item Embedding编码。

在训练过程中，User Embedding和Item Embedding做内积或者Cosine相似度计算（注:**Cosine相当于对User Embedding和Item Embedding内积基础上，进行了两个向量模长归一化，只保留方向一致性不考虑长度**），使得用户和正例Item在Embedding空间更接近，和负例Item在Embedding空间距离拉远。**损失函数则可用标准交叉熵损失，将问题当作一个分类问题，或者类似DSSM采取BPR或者Hinge Loss，将问题当作一个表示学习问题**。

虽说上图两个塔的**DNN模块介绍说的是MLP结构，但是理论上这里可以替换成任意你想用的模型结构，比如Transformer或者其它模型，最简单的应该是FM模型，如果这里用FM模型做召回或者粗排，等于把上图的DNN模块替换成了对特征Embedding进行“Sum”求和操作，貌似应该是极简的双塔模型了**。所以说，双塔结构不是一种具体的模型结构，而是一种抽象的模型框架。

一般在推荐的模型召回环节应用双塔结构的时候，分为**离线训练和在线应用**两个环节。

上面基本已描述了离线训练过程，至于**在线应用，一般是这么用的：**

- 首先，**通过训练数据，训练好User侧和Item侧两个塔模型，我们要的是训练好后的这两个塔模型，让它们各自接受用户或者Item的特征输入，能够独立打出准确的User Embedding或者Item Embedding**

- 之后，**对于海量的候选Item集合，可以通过Item侧塔，离线将所有Item转化成Embedding，并存储进ANN检索系统**，比如FAISS，以供查询。**为什么双塔结构用起来速度快？主要是类似FAISS这种ANN检索系统对海量数据的查询效率高**

- 再往后，某个用户的User Embedding，一般要求实时更新，以体现用户最新的兴趣。为了达成实时更新，有以下不同的做法：
  - 比如你可以通过在线模型来实时更新双塔的参数来达成这一点，**这是在线模型的路子**；
  - 但是很多情况下，并非一定要采取在线模型，毕竟实施成本高，而**可以固定用户侧的塔模型参数，采用在输入端，将用户最新行为过的Item做为用户侧塔的输入，然后通过User侧塔打出User Embedding**，这种模式。这样也可以实时地体现用户即时兴趣的变化，**这是特征实时的角度**，做起来相对简单。

- 最后，**有了最新的User Embedding，就可以从FAISS库里拉取相似性得分Top K的Item，做为个性化召回结果。**

#### NeuralCF 模型的扩展：双塔模型

- NeuralCF 的模型结构之中，蕴含了一个非常有价值的思想，就是我们**可以把模型分成用户侧模型和物品侧模型两部分，然后用互操作层把这两部分联合起来，产生最后的预测得分**。这里的用户侧模型结构和物品侧模型结构，可以是简单的 Embedding 层，也可以是复杂的神经网络结构，最后的互操作层可以是简单的点积操作，也可以是比较复杂的 MLP 结构。但**只要是这种物品侧模型 + 用户侧模型 + 互操作层的模型结构，我们把它统称为“双塔模型”结构。**
- 图 4 就是一个典型“双塔模型”的抽象结构。它的名字形象地解释了它的结构组成，两侧的模型结构就像两个高塔一样，而最上面的互操作层则像两个塔尖搭建起的空中走廊，负责两侧信息的沟通。

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/25-4.png" style="zoom:33%;" />

- 对于 NerualCF 来说，它只利用了用户 ID 作为“用户塔”的输入特征，用物品 ID 作为“物品塔”的输入特征。事实上，我们完全**可以把其他用户和物品相关的特征也分别放入用户塔和物品塔，让模型能够学到的信息更全面。比如说，YouTube 在构建用于召回层的双塔模型时，就分别在用户侧和物品侧输入了多种不同的特征，如图 5 所示。**

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/25-5.png)

- YouTube 召回双塔模型的**用户侧特征包括了用户正在观看**的视频 ID、频道 ID（图中的 seed features）、该视频的观看数、被喜欢的次数，以及用户历史观看过的视频 ID 等等。**物品侧的特征包括了候选视频的** ID、频道 ID、被观看次数、被喜欢次数等等。在经过了多层 ReLU 神经网络的学习之后，双塔模型最终通过 softmax 输出层连接两部分，输出最终预测分数。
- 这个双塔模型相比我们之前学过的 Embedding MLP 和 Wide&Deep 有什么优势呢？其实**在实际工作中，双塔模型最重要的优势就在于它易上线、易服务。为什么这么说呢？**

- 物品塔和用户塔最顶端的那层神经元，那层神经元的输出其实就是一个全新的物品 Embedding 和用户 Embedding。拿图 1 来说，物品塔的输入特征向量是 x，经过物品塔的一系列变换，生成了向量 u(x)，那么这个 u(x) 就是这个物品的 Embedding 向量。同理，v(y) 是用户 y 的 Embedding 向量，这时，我们就可以把 u(x) 和 v(y) 存入特征数据库，这样一来，**线上服务的时候，我们只要把 u(x) 和 v(y) 取出来，再对它们做简单的互操作层运算就可以得出最后的模型预估结果了！所以使用双塔模型，我们不用把整个模型都部署上线，只需要预存物品塔和用户塔的输出，以及在线上实现互操作层就可以了**。如果这个互操作层是点积操作，那么这个实现可以说没有任何难度，这是实际应用中非常容易落地的，这也正是双塔模型在业界巨大的优势所在

