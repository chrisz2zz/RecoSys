#### FM和MF的联系

MF（Matrix Factorization，矩阵分解）模型是个在推荐系统领域里资格很深的协同过滤模型了。**核心思想是通过两个低维小矩阵（一个代表用户embedding矩阵，一个代表物品embedding矩阵）的乘积计算，来模拟真实用户点击或评分产生的大的协同信息稀疏矩阵，本质上是编码了用户和物品协同信息的降维模型**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22-6.png" style="zoom: 33%;" />

当训练完成，每个用户和物品得到对应的低维embedding表达后，如果要预测某个 $User_i$ 对 $Item_j$ 的评分的时候，只要它们做个内积计算 $<User_i,Item_j>$ ，这个得分就是预测得分

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22-7.png" style="zoom:33%;" />

**MF到FM的转换**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22-8.png" style="zoom:50%;" />

本质上，**MF模型是FM模型的特例，MF可以被认为是只有User ID 和Item ID这两个特征Fields的FM模型**，MF将这两类特征通过矩阵分解，来达到将这两类特征embedding化表达的目的。

**而FM则可以看作是MF模型的进一步拓展，除了User ID和Item ID这两类特征外，很多其它类型的特征，都可以进一步融入FM模型里**，它**将所有这些特征转化为embedding低维向量表达，并计算任意两个特征embedding的内积，就是特征组合的权重**

FM继承了MF的特征embedding化表达这个优点，同时**引入了更多Side information作为特征，将更多特征及Side information embedding化融入FM模型中**

#### FM的复杂度优化

- 优化**目标**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-5.png" style="zoom:33%;" />

- 总体概览

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-6.png" style="zoom:33%;" />

- Step1 步骤拆解

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-7.png" style="zoom:50%;" />

- Step 2

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-8.png" style="zoom:50%;" />

- step 3

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-9.png" style="zoom:50%;" />

- Step3-1

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/24-1.png" style="zoom:50%;" />

- Step3-2

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/24-2.png" style="zoom:50%;" />

- Step4

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/24-3.png" style="zoom:50%;" />

通过上述四步的公式改写，可以看出在实现FM模型时，时间复杂度就降低到了 $O(k * n) $了，而虽说看上去n还有点大，但是其实真实的推荐数据的特征值是极为稀疏的，就是说大量xi其实取值是0，意味着真正需要计算的特征数n是远远小于总特征数目n的，无疑这会进一步极大加快FM的运算效率。

这里需要强调下**改写之后的FM公式的第一个平方项，怎么理解这个平方项的含义呢？这里其实蕴含了后面要讲的使用FM模型统一多路召回的基本思想，所以这里特殊提示一下**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/24-4.png" style="zoom:50%;" />

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/24-5.png" style="zoom:50%;" />

**这个平方项，它等价于将FM的所有特征项的embedding向量累加，之后求内积。会在下面做召回时用到**



#### **如何用FM模型做召回模型**

如果要做一个实用化的统一召回模型，要考虑的因素有很多，比如Context上下文特征怎么处理，实时反馈特征怎么加入等。为了能够更清楚地说明，我们**先从极简模型说起，然后逐步加入必须应该考虑的元素，最后形成一个实用化的统一召回模型。**

不论是简化版本FM召回模型，还是复杂版本，首先都需要做如下两件事情：

**第一，离线训练**。这个过程跟在排序阶段采用FM模型的离线训练过程是一样的，比如**可以使用线上收集到的用户点击数据来作为训练数据，线下训练一个完整的FM模型。涉及到负采样**

- **在召回阶段，我们想要的其实是：每个特征和这个特征对应的训练好的embedding向量。这个可以存好待用。**

**第二，如果将推荐系统做个很高层级的抽象的话，可以表达成学习如下形式的映射函数：**
$$
y = F(User, Item, Context)
$$

- 上述即是：我们利用用户（User）相关的特征，物品(Item)相关的特征，以及上下文特征（Context：比如何时何地用的什么牌子手机登陆等等）学习一个映射函数F。**学好这个函数后，当以后新碰到一个Item，我们把用户特征，物品特征以及用户碰到这个物品时的上下文特征输入F函数，F函数会告诉我们用户是否对这个物品感兴趣。如果他感兴趣，就把这个Item作为推荐结果推送给用户。**

第二个需要做的事情是：**把特征划分为三个子集合，用户相关特征集合，物品相关特征集合以及上下文相关的特征集合**。而用户历史行为类特征，比如用户过去点击物品的特征，可以当作描述用户兴趣的特征，放入用户相关特征集合内。至于为何要这么划分，后面会讲。

做完上述两项基础工作，我们可以试着用FM模型来做召回了

#### 极简版FM召回模型

先来构建一个极简的FM召回模型，首先，我们先不考虑上下文特征

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/22-9.png" style="zoom:33%;" />

第一步，**对于某个用户，我们可以把属于这个用户子集合的特征，查询离线训练好的FM模型对应的特征embedding向量，然后将n个用户子集合的特征embedding向量累加，形成用户兴趣向量U，这个向量维度和每个特征的维度是相同的**。

类似的，**我们也可以把每个物品，其对应的物品子集合的特征，查询离线训练好的FM模型对应的特征embedding向量，然后将m个物品子集合的特征embedding向量累加，形成物品向量I，这个向量维度和每个特征的维度也是是相同的**。

对于极简版FM召回模型来说：

- 用户兴趣向量U可以离线算好，然后更新线上的对应内容；物品兴趣向量I可以类似离线计算或者近在线计算，问题都不大

整个的流程如下图：

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-1.png" style="zoom:50%;" />

**第二步，对于每个用户以及每个物品，我们可以利用步骤一中的方法，将每个用户的兴趣向量离线算好，存入在线数据库中比如Redis（用户ID及其对应的embedding），把物品的向量逐一离线算好，存入Faiss(Facebook开源的embedding高效匹配库)数据库中。**

当用户登陆或者刷新页面时，可以**根据用户ID取出其对应的兴趣向量embedding，然后和Faiss中存储的物料embedding做内积计算，按照得分由高到低返回得分Top K的物料作为召回结果**。提交给第二阶段的排序模型进行进一步的排序。

这样就完成了一个极简版本FM召回模型。但是**这个版本的FM召回模型存在两个问题**：

问题一：**这种累加用户embedding特征向量以及累加物品embedding特征向量，之后做向量内积。这种算法符合FM模型的原则吗？和常规的FM模型是否等价？**

我们来分析一下。**这种做法其实是在做用户特征集合U和物品特征集合I之间两两特征组合，是符合FM的特征组合原则的**，考虑下列公式是否等价就可以明白了：
$$
\begin{array}{l}\left\langle\sum_{i} U_{i}, \sum_{j} I_{j}\right\rangle \text { (公式1) } \\ \sum_{i} \sum_{j}\left\langle U_{i}, I_{j}\right\rangle \quad(\text { 公式2) }\end{array}
$$
当然，**跟完全版本的FM比，我们没有考虑U和I特征集合内部任意两个特征的组合**

也可以这么思考问题：在上文我们说过，**FM为了提升计算效率，对公式进行了改写**，改写后的**高效计算公式的第一个平方项其实等价于：把所有特征embedding向量逐位累加成一个求和向量V，然后自己和自己做个内积操作<V,V>**。这样**等价于根据FM的原则计算了任意两个特征的二阶特征组合了**。

而上面描述的方法，和标准的FM的做法其实是一样的，区别无非是将特征集合划分为两个子集合U和I，分别代表用户相关特征及物品相关特征。而**上述做法其实等价于在用户特征和物品特征之间做两两特征组合，只是少了U内部之间特征，及I内部特征之间的特征组合而已**。

一般而言，**其实我们不需要做U内部特征之间以及I内部特征之间的特征组合，对最终效果影响很小。于是，沿着这个思考路径，我们也可以推导出上述做法基本和FM标准计算过程是等价的。**

#### 加入场景上下文特征

抽象的推荐系统除了用户特征及物品特征外，**还有一类重要特征，就是用户发生行为的场景上下文特征**（比如什么时间在什么地方用的什么设备在刷新），而上面版本的召回模型并没有考虑这一块。

之所以把上下文特征单独拎出来，是因为它有自己的特点，**有些上下文特征是近乎实时变化的，比如刷新微博的时间，再比如对于美团滴滴这种对地理位置特别敏感的应用，用户所处的地点可能随时也在变化，而这种变化在召回阶段就需要体现出来**。

**所以，上下文特征是不太可能像用户特征离线算好存起来直接使用的，而是用户在每一次刷新可能都需要重新捕获当前的特征值。动态性强是它的特点。**

而考虑进来上下文特征，如果我们希望构造和标准的FM等价的召回模型，就需要多考虑两个问题：

- 问题一：**既然部分上下文特征可能是实时变化的，无法离线算好，那么怎么融入上文所述的召回计算框架里？**

- 问题二：我们**需要考虑上下文特征C和用户特征U之间的特征组合，也需要考虑C和物品特征I之间的特征组合。上下文特征有时是非常强的特征。那么，如何做能够将这两对特征组合考虑进来呢？**

我们可以这么做：

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-2.png" style="zoom:33%;" />

1. **由于上下文特征的动态性，所以给定用户UID后，可以在线查询某个上下文特征对应的embedding向量**，然后**所有上下文向量求和得到综合的上下文向量C。这个过程其实和U及I的累加过程是一样的，区别无非是上下文特征需要在线实时计算**。而一般而言，场景上下文特征数都不多，所以在线计算，速度方面应可接受

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-3.png" style="zoom:33%;" />

2. **将在线算好的上下文向量C和这个用户的事先算好存起来的用户兴趣向量U进行内积计算Score=<U,C>。这个数值代表用户特征和上下文特征的二阶特征组合得分**，算好备用。至于为何这个得分能够代表FM中的两者（U和C）的特征组合，其实道理和上面讲的U和I做特征组合道理是一样的。

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/23-4.png" style="zoom:33%;" />

3. 将**U和C向量累加求和，利用（U+C）去Faiss通过内积方式取出Top K物品，这个过程和极简版是一样的**，无非查询向量由U换成了（U+C）。**通过这种方式取出的物品同时考虑到了用户和物品的特征组合<U,I>，以及上下文和物品的特征组合<C,I>**。道理和之前讲的内容是类似的
4. **假设返回的Top K物品都带有内积的得分Score1，再考虑上一步<U,C>的得分Score，将两者相加对物品重排序**（<U,C>因为跟物品无关，所以其实不影响物品排序，但是会影响最终得分，FM最外边的Sigmoid输出可能会因为加入这个得分而发生变化），就得到了最终结果，**而这个最终结果考虑了U/I/C两两之间的特征组合。**
5. 于是我们通过这种手段，构造出了一个完整的FM召回模型。这**个召回模型通过构造user embedding，Context embedding和Item embedding，以及充分利用类似Faiss这种高效embedding计算框架，就构造了高效执行的和FM计算等价的召回系统**。

#### FM做召回和排序的不同

**我们不能只训练一个FM排序模型 ，然后直接拿这个排序模型用于召回**。尽管都是基于FM算法，但是FM召回与排序，有以下不同：

- **使用的特征不同** 

- - **FM召回**，由于未来要依赖Faiss进行线上检索，**所以不能使用user与doc的交叉特征**。只有如此，我们才能独立计算user embedding与doc embedding
  - **FM排序**，则没有这方面的限制，**可以使用user与doc的交叉特征**。因为FM所实现自动二阶交叉，仅能代表“共现”。但是user与doc之间还有其他形式的交叉，比如user tag与doc tag之间的重合度，喂入这样的交叉，对于排序性能提升，仍然有很大帮助

- **使用的样本不同** 

- - 训练FM做排序时，必须**使用“曝光未点击”这样的“真负”样本**。
  - 训练FM做召回时，起码**不能只使用“曝光未点击”做负样本。大部分的负样本必须通过随机采样得到**

- **使用的Loss不同** 

- - FM排序时，由于负样本是真实的，可以采用CTR预估那样的point-wise loss
  - FM召回时，由于负样本是随机采样得到的，存在一定的噪声，最好采用BPR, hinge这样的pair-wise loss

