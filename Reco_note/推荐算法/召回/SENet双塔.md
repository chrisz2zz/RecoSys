#### 双塔模型

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/28-1.png" style="zoom:33%;" />

双塔模型结构非常简单，如上图所示，左侧是用户塔，右侧是Item塔，可将特征拆分为两大类：

- **用户相关特征**（用户基本信息、群体统计属性以及行为过的Item序列等）
- **Item相关特征**（Item基本信息、属性信息等）
- 原则上，**Context上下文特征可以放入用户侧塔**

对于这两个塔本身，则是经典的DNN模型，从特征OneHot到特征Embedding，再经过几层MLP隐层，两个塔分别输出用户Embedding和Item Embedding编码。

在训练过程中，User Embedding和Item Embedding做内积或者Cosine相似度计算（注:**Cosine相当于对User Embedding和Item Embedding内积基础上，进行了两个向量模长归一化，只保留方向一致性不考虑长度**），使得用户和正例Item在Embedding空间更接近，和负例Item在Embedding空间距离拉远。**损失函数则可用标准交叉熵损失，将问题当作一个分类问题，或者类似DSSM采取BPR或者Hinge Loss，将问题当作一个表示学习问题**。

虽说上图两个塔的**DNN模块介绍说的是MLP结构，但是理论上这里可以替换成任意你想用的模型结构，比如Transformer或者其它模型，最简单的应该是FM模型，如果这里用FM模型做召回或者粗排，等于把上图的DNN模块替换成了对特征Embedding进行“Sum”求和操作，貌似应该是极简的双塔模型了**。所以说，双塔结构不是一种具体的模型结构，而是一种抽象的模型框架。

一般在推荐的模型召回环节应用双塔结构的时候，分为**离线训练和在线应用**两个环节。

上面基本已描述了离线训练过程，至于**在线应用，一般是这么用的：**

- 首先，**通过训练数据，训练好User侧和Item侧两个塔模型，我们要的是训练好后的这两个塔模型，让它们各自接受用户或者Item的特征输入，能够独立打出准确的User Embedding或者Item Embedding**

- 之后，**对于海量的候选Item集合，可以通过Item侧塔，离线将所有Item转化成Embedding，并存储进ANN检索系统**，比如FAISS，以供查询。**为什么双塔结构用起来速度快？主要是类似FAISS这种ANN检索系统对海量数据的查询效率高**

- 再往后，某个用户的User Embedding，一般要求实时更新，以体现用户最新的兴趣。为了达成实时更新，有以下不同的做法：
  - 比如你可以通过在线模型来实时更新双塔的参数来达成这一点，**这是在线模型的路子**；
  - 但是很多情况下，并非一定要采取在线模型，毕竟实施成本高，而**可以固定用户侧的塔模型参数，采用在输入端，将用户最新行为过的Item做为用户侧塔的输入，然后通过User侧塔打出User Embedding**，这种模式。这样也可以实时地体现用户即时兴趣的变化，**这是特征实时的角度**，做起来相对简单。

- 最后，**有了最新的User Embedding，就可以从FAISS库里拉取相似性得分Top K的Item，做为个性化召回结果。**

#### 双塔的问题

双塔结构有一个内生性的问题，就是它的**结构必然会导致精度的损失**，为什么这么说呢？我们再审视下双塔的结构，它最大的特点是：首**先对用户特征和Item特征分离，两组分离的特征，各自通过DNN网络进行信息集成，集成结果就是两个塔各自顶端的User Embedding和Item Embedding**。相比我们平常在精排阶段见到的DNN模型，这种**特征分离**的设计，会带来两个问题：

- 第一个问题：我们一般在做推荐模型的时候，会有些特征工程方面的工作，比如设计一些User侧特征和Item侧特征的组合特征，一般而言，这种来自User和Item两侧的组合特征是非常有效的判断信号。**但是，如果我们采用双塔结构，这种人工筛选的，来自两侧的特征组合就不能用了，因为它既不能放在User侧，也不能放在Item侧，这是特征工程方面带来的效果损失**。当然，我个人认为，这个问题不是最突出的，应该有办法绕过去，或者模型能力强，把组合特征拆成两个分离的特征，各自放在对应的两侧，可以让模型去捕获这种组合特征。

- 第二个问题：如果是精排阶段的DNN模型，来自User侧和Item侧的特征，在很早的阶段，比如第一层MLP隐层，两者之间就可以做细粒度的特征交互。**但是，对于双塔模型，两侧特征什么时候才能发生交互？只有在User Embedding和Item Embedding发生内积的时候，两者才发生交互，而此时的User Embedding和Item Embedding，已经是两侧特征经过多次非线性变换，糅合出的一个表征用户或者Item的整体Embedding了，细粒度的特征此时估计已经面目模糊了，就是说，两侧特征交互的时机太晚了**。我们知道，User侧和Item侧特征之间的交互，是非常有效的判断信号。而很多领域的实验已经证明，双塔这种过晚的两侧特征交互，相对在网络结构浅层就进行特征交互，会带来效果的损失。这个问题比较严重。

这就是为何我们说，双塔结构的内生性问题。归纳起来，就是说：**为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生的效果损失**，那么，我们能怎么改造这种结构，让效果损失的少一些呢？

#### SENet用于推荐

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/28-2.png" style="zoom:50%;" />

SENet由自动驾驶公司Momenta在2017年提出，在当时，是一种应用于图像处理的新型网络结构。**它基于CNN结构，通过对特征通道间的相关性进行建模，对重要特征进行强化来提升模型准确率，本质上就是针对CNN中间层卷积核特征的Attention操作。**

推荐领域里面的特征有个特点，就是海量稀疏，意思是大量长尾特征是低频的，而这些低频特征，去学一个靠谱的Embedding是基本没希望的，但是你又不能把低频的特征全抛掉，因为有一些又是有效的。既然这样，如果**我们把SENet用在特征Embedding上，类似于做了个对特征的Attention，弱化那些不靠谱低频特征Embedding的负面影响，强化靠谱低频特征以及重要中高频特征的作用**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/28-3.png" style="zoom:33%;" />

如上图所示， 标准的DNN模型一般有一个**特征Embedding层，我们可以把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些特征的重要性 $\alpha_i$ ：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的**。

SENet分为两个步骤：Squeeze 阶段和Excitation阶段。

**在Squeeze阶段**，我们对每个特征的Embedding向量进行数据压缩与信息汇总，如下：
$$
z_{i}=F_{s} q\left(v_{i}\right)=1 / k \sum_{(t=1)}^{k} v_{i}^{t}
$$
其实很简单，就是说假设**某个特征 $v_i$ 是k维大小的Embedding，那么我们对Embedding里包含的k维数字求均值，得到能够代表这个特征汇总信息的数值 $z_i$ ，也就是说，把第i个特征的Embedding里的信息压缩到一个数值**。

原始版本的SENet，在这一步是对CNN的二维卷积核进行Max操作的，我们这里等于对某个特征Embedding元素求均值。我们试过，在推荐领域均值效果比Max效果好，这也很好理解，因为图像领域对卷积核元素求Max，等于找到最强的那个特征，而推荐领域的特征Embedding，每一位的数字都是有意义的，所以求均值能更好地保留和融合信息。

最终，**经过Squeeze阶段之后，对于每个特征 $v_i$ ，都压缩成了单个数值 $z_i$ ，假设特征Embedding层有 $f$ 个特征，就形成了Squeeze向量 $Z$，向量大小为 $f$。**

**在Excitation阶段**，我们引入了中间层比较窄的**两层MLP网络，作用在Squeeze阶段的输出向量 $Z$ 上**，如下：
$$
S=F_{e x}(Z, W)=\delta\left(W_{2} \delta\left(W_{1} Z\right)\right)
$$
$\delta$ 是非线性函数，一般取Relu。本质上，这是在做特征的交叉，也就是说，每个特征以一个Bit来表征，通过MLP来进行交互，通过交互，得出这么个结果：对于当前所有输入的特征，通过相互发生关联，来动态地判断哪些特征重要，哪些特征不重要。

- 其中，**第一个MLP的作用是做特征交叉，第二个MLP的作用是为了保持输出的大小维度**。因为假设Embedding层有 $f$ 个特征，那么我们需要保证输出 $f$ 个权重值，而第二个MLP就是起到将大小映射到 $f$ 个数值大小的作用。
- **这样，经过两层MLP映射，就会产生 $f$ 个权重数值，第 $i$ 个数值对应第 $i$ 个特征Embedding的权重 $a_i$ 。我们把每个特征对应的权重 $a_i$ ，再乘回到特征对应的Embedding里，就完成了对特征重要性的加权操作**。  $a_i$ 数值大，说明SENet判断这个特征在当前输入组合里比较重要， $a_i$ 数值小，说明SENet判断这个特征在当前输入组合里没啥用。如果非线性函数用Relu，你会发现大量特征的权重会被Relu搞成0，也就是说，其实很多特征是没啥用的。

这样，我们就可以将SENet引入推荐系统，用来对特征重要性进行动态判断。注意，所谓动态，指的是比如对于某个特征，在某个输入组合里可能是没用的，但是换一个输入组合，很可能是重要特征。它重要不重要，不是静态的，而是要根据当前输入，动态变化的

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/28-4.png" style="zoom:33%;" />

参考上图，其实很简单，**就是在用户侧塔和Item侧塔，在特征Embedding层上，各自加入一个SENet模块就行了，两个SENet各自对User侧和Item侧的特征，进行动态权重调整，强化那些重要特征，弱化甚至清除掉**（如果权重为0的话）不重要甚至是噪音的特征。其余部分和标准双塔模型是一样的

#### FM与双塔的比较

相对FM模型，**双塔DNN的优点是引入了非线性**，但是因为**这种非线性是在User侧特征之间，或者Item侧特征之间做的，所以可能发挥的作用就没有期待中那么大，因为User侧和Item侧之间的特征交互会更有效一些**。而**单侧特征的多层非线性操作，可能反而会带来上面说的两侧特征交互太晚，细节信息损失的问题。**

而FM模型，感觉特性和DNN双塔正好相反，它**在User侧和Item侧交互方面比较有优势，因为没有深层，也没有非线性对单侧特征的深度融合，只在两侧特征Embedding层级发生交互作用，所以在特征Embedding层级能够更好地表达User侧和Item侧特征之间的交叉作用，当然，缺点是缺乏非线性。**所以，如果仔细分析的话，会发现FM和DNN双塔，是各有擅长之处的。

再来看为何把SENet引入双塔模型会是有效的？以下是张俊林老师的原话

>  我推测可能的原因是：它很可能集成了FM和DNN双塔各自的优点，在User侧和Item侧特征之间的交互表达方面增强了DNN双塔的能力。**SENet通过参数学习，动态抑制User或者Item内的部分低频无效特征，很多特征甚至被清零，这样的好处是，它可以凸显那些对高层User Embedding和Item Embedding的特征交叉起重要作用的特征，更有利于表达两侧的特征交互，避免单侧无效特征经过DNN双塔非线性融合时带来的噪声，同时，它又带有非线性的作用**。这貌似能同时吸收了FM和DNN各自的优势，取得一个折衷效果。当然，这只是个人推测

