#### KNN与ANN

为什么要用向量快速检索呢？因为实际上现在各家公司主召回都会使用向量化召回，但是工业界数据规模太大，精确的近邻搜索太过困难，研究随之转向了在精确性和搜索时间做取舍，即Approximate Nearest Neighbor Search (ANNS)

向量检索就是在一个给定向量数据集中，按照某种度量方式，检索出与查询向量相近的K个向量（K-Nearest Neighbor，KNN），但由于KNN计算量过大，我们通常只关注近似近邻（Approximate Nearest Neighbor，ANN）问题

#### 向量相似度度量的几种距离

##### 欧式距离

- 二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：

$$
d_{12}=\sqrt{\left(x_{1}-x_{2}\right)^{2}+\left(y_{1}-y_{2}\right)^{2}}
$$

- 最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 x = (x1,...,xn) 和 y = (y1,...,yn) 之间的距离为：

$$
\operatorname{dist}(X, Y)=\sqrt{\sum_{i=1}^{n}\left(x_{i}-y_{i}\right)^{2}}
$$

##### 曼哈顿距离

- 二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离

$$
d_{12} = |x_1 - x_2| + |y_1 - y_2|
$$

##### 余弦相似度

- 范围：[-1,1]

$$
\begin{array}{c}\mathbf{a} \cdot \mathbf{b}=\|\mathbf{a}\|\|\mathbf{b}\| \cos \theta . \\ \text { similarity }=\cos (\theta)=\frac{A \cdot B}{\|A\|\|B\|}=\frac{\sum_{i=1}^{n} A_{i} \times B_{i}}{\sqrt{\sum_{i=1}^{n}\left(A_{i}\right)^{2}} \times \sqrt{\sum_{i=1}^{n}\left(B_{i}\right)^{2}}}\end{array}
$$

##### 余弦距离

- 范围：[0,2] 

$$
distance = 1 - 余弦相似度
$$

##### 内积

$$
\mathbf{a} \cdot \mathbf{b}=\sum_{i=1}^{n} a_{i} \times b_{i}
$$

#### Faiss

向量计算是一个最经典的时空优化问题，在查询过程中建立更多地索引固然可以提升查询速度，但是却有占据了存储空间，我们希望系统可以即减少索引又能提升查询性能。

为了得到时间和空间的最优，**Faiss使用了PCA和PQ两个手段进行向量压缩和编码**，当然还有其它的一些优化手段，但是PCA和PQ是最为核心的。

- PCA：**PCA是一种降维手段，简单理解就是将高维向量变为低维**，这样就可以有效的节省存储空间
- PQ：**Product quantization(乘积量化PQ)，PQ是一种建立索引的方式**

#### kd_tree

- **为空间中的点、向量建立一个索引**
- kd 树是一种对k维特征空间中的实例点进行存储以便对其快速检索的树形数据结构
- **kd树是二叉树，会选取向量中某个方差最大的维度 (即该维变化最剧烈的) 去划分，也就是以超平面去划分空间；核心思想是对 k 维特征空间不断切分（假设特征维度是768，对于(0,1,2,...,767)中的每一个维度，以中值递归切分）构造的树，每一个节点是一个超矩形**，小于结点的样本划分到左子树，大于结点的样本划分到右子树
- 树构造完毕后，最终检索时（1）**从根结点出发，递归地向下访问kd树**。若目标点 ![[公式]](https://www.zhihu.com/equation?tex=x) **当前维**的坐标小于切分点的坐标，移动到左子树，否则移动到右子树，直至到达叶结点；（2）以此叶结点为“最近点”，**递归地向上回退，查找该结点的兄弟结点中是否存在更近的点**，若存在则更新“最近点”，否则回退；未到达根结点时继续执行（2）；（3)**回退到根结点时，搜索结束**
- **kd树在维数小于20时效率最高**，一般适用于训练实例数远大于空间维数时的k近邻搜索；当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线形扫描

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-2.png" style="zoom: 33%;" />

如下图中的点云，先用红色的线把点云一分为二，再用深蓝色的线把各自片区的点云一分为二，以此类推，直到每个片区只剩下一个点，这就完成了空间索引的构建。比如，**希望找到点 q 的 m 个邻接点，我们就可以先搜索它相邻子树下的点，如果数量不够，我们可以向上回退一个层级，搜索它父片区下的其他点，直到数量凑够 m 个为止**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-1.png" style="zoom:33%;" />

缺点：

还是**无法完全解决边缘点最近邻的问题**。对于点 q 来说，它的邻接片区是右上角的片区，但是它的最近邻点却是深蓝色切分线下方的那个点。所以**按照 Kd-tree 的索引方法，我们还是会遗漏掉最近邻点，它只能保证快速搜索到近似的最近邻点集合**。而且 Kd-tree 索引的结构并不简单，离线和在线维护的过程也相对复杂

#### Annoy

- 类似KD树，但是不同的是**annoy没有对k维特征进行切分。annoy的每一次空间划分，可以看作聚类数为2的KMeans过程**

- **建立索引过程；**

  - **annoy的每一次空间划分，可以看作聚类数为2的KMeans过程：**随机抽取两个点执行k=2的k均值聚类，分成两簇，收敛后产生两个聚类中心。两个聚类中心连一条灰色线，建立一条垂直于灰线，通过灰线中心点的黑线，黑色粗线将数据空间分成两部分。多维空间中，这条黑线可以看做等距垂直超平面

  <img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-3.png" style="zoom:50%;" />

  - **多次递归迭代划分的话，最终原始数据会形成一个二叉树结构，树的叶子节点存原始数据结点，非叶子节点存放分割线（黑线）信息。Annoy建立这样的二叉树结构是希望满足这样的一个假设: 相似的数据节点应该在二叉树上位置更接近，一个分割超平面不应该把相似的数据节点分割二叉树的不同分支上**

  <img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-4.png" style="zoom: 33%;" />

- **近邻查询过程：**

  - **查找的过程就是不断看他在分割超平面的哪一边**。从二叉树索引结构来看，**就是从根节点不停的往叶子节点遍历的过程。通过对二叉树每个中间节点（分割超平面相关信息）和查询数据节点进行相关计算来确定二叉树遍历过程是往这个中间节点左孩子节点走还是右孩子节点走**。通过以上方式完成查询过程

  <img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-5.png" style="zoom:50%;" />

- 查询中遇到的问题：

  - 查询过程**最终落到叶子节点的数据节点数小于我们需要的TopN相似邻居节点数目**
    - 解决方法：**建多棵树，即森林**

  <img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-6.png" style="zoom:33%;" />

  - **两个相近的数据节点划分到二叉树不同分支上**
    - 解决方法：**分割超平面的两边都很相似，那可以两边都遍历**

  <img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-7.png" style="zoom: 33%;" />

- **返回最终近邻节点；**每棵树都返回一堆近邻点后，如何得到最终的Top N相似集合

  - 所有树返回近邻点都插入到优先队列（最大的先出或最小的先出）中，求并集去重
  - 然后计算和查询点距离
  - 最终根据距离值从近距离到远距离排序， 返回Top N近邻节点集合

- **一般树越多，精准率越高但是对内存的开销也越大，需要权衡取舍**



#### NSW

NSW（Navigable Small World graphs）是基于图存储的数据结构

比如下图中，最朴素的查找法是：**当我想查找与粉色点最近的一点时，我从任意一个黑色点出发，计算它和粉色点的距离，与这个任意黑色点有连接关系的点我们称之为“友点”（直译），然后我要计算这个黑色点的所有“友点”与粉色点的距离，从所有“友点”中选出与粉色点最近的一个点，把这个点作为下一个进入点，继续按照上面的步骤查找下去。如果当前黑色点对粉色点的距离比所有“友点”都近，终止查找，这个黑色点就是我们要找的离粉色点最近的点**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-8.png" style="zoom: 33%;" />

**缺点：**

朴素想法之所以叫朴素想法就是因为它的缺点非常多。首先，**我们发现图中的K点是无法被查询到的，因为K点没有友点**，怎么办？。其次，如果我们要查找距离粉色点最近的两个点，而**这两个近点之间如果没有连线，那么将大大影响效率（比如L和E点，如果L和E有连线，那么我们可以轻易用上述方法查出距离粉色点最近的两个点）**，怎么办？。最后一个大问题，D点真的需要这么多“友点”吗？谁是谁的友点应该怎么确定呢？

解决思路：

- 关于K点的问题（孤点无法被找到），我们**规定在构图时所有数据向量节点都必须有友点**。
- 关于L和E的问题（近点必须有连线），我们**规定在构图时所有距离相近（相似）到一定程度的向量必须互为友点**。
- 关于D点问题（友点数目要控制），权衡构造这张图的时间复杂度，我们**规定尽量减少每个节点的“友点”数量**。

在图论中有一个很好的剖分法则专门解决上一节中提到的朴素想法的缺陷问题------**德劳内（Delaunay）三角剖分算法**，这个算法可以达成如下要求：1，图中每个点都有“友点”。2，相近的点都互为“友点”。3，图中所有连接（线段）的数量最少。效果如下图

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26-9.png" style="zoom:33%;" />

但NSW没有采用德劳内三角剖分法来构成德劳内三角网图，原因之一是**德劳内三角剖分构图算法时间复杂度太高**，换句话说，构图太耗时。原因之二是**德劳内三角形的查找效率并不一定最高，如果初始点和查找点距离很远的话我们需要进行多次跳转才能查到其临近点**，需要“**高速公路”机制**（Expressway mechanism, 这里指**部分远点之间拥有线段连接，以便于快速查找**）。在理想状态下，我们的算法不仅要满足上面三条需求，还要算法复杂度低，同时配有高速公路机制的构图法

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/27-1.png" style="zoom:33%;" />

NSW论文中配了这样一张图，**黑色是近邻点的连线，红色线就是“高速公路机制”了**。我们从enter point点进入查找，查找绿色点临近节点的时候，就可以用过红色连线“高速公路机制”快速查找到结果。

**NSW朴素构图算法在这里：向图中逐个插入点，插图一个全新点时，通过朴素想法中的朴素查找法（通过计算“友点”和待插入点的距离来判断下一个进入点是哪个点）查找到与这个全新点最近的m个点（m由用户设置），连接全新点到m个点的连线**。

在上面这个戛然而止的算法描述中有些读者肯定会问，就这么简单？对，就这么简单。我们来理智地分析一下这个算法。**首先，我们的构图算法是逐点随机插入的，这就意味着在图构建的早期，很有可能构建出“高速公路”。**假设我们现在要构成10000个点组成的图，设置m=4（每个点至少有4个“友点”），这10000个点中有两个点，p和q，他们俩坐标完全一样。假设在插入过程中我们分别在第10次插入p，在第9999次插入q，请问p和q谁更容易具有“高速公路”？答：因为在第10次插入时，只见过前9个点，故只能在前9个点中选出距离最近的4个点（m=4）作为“友点”，而q的选择就多了，前9998个点都能选，所以q的“友点”更接近q，p的早期“友点”不一定接近p，所以p更容易具有“高速公路”。

结论：**一个点，越早插入就越容易形成与之相关的“高速公路”连接，越晚插入就越难形成与之相关的“高速公路”连接。所以这个算法设计的妙处就在于扔掉德劳内三角构图法，改用“无脑添加”（NSW朴素插入算法），降低了构图算法时间复杂度的同时还带来了数量有限的“高速公路”，加速了查找。**

 下面对NSM朴素构图算法的过程举例

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/27-2.png" style="zoom:33%;" />

我们对7个二维点进行构图，用户设置m=3（每个点在插入时找3个紧邻友点）。

- 首先初始点是A点（随机出来的），A点插入图中只有它自己，所以无法挑选“友点”。
- 然后是B点，B点只有A点可选，所以连接BA，此为第1次构造。
- 然后插入F点，F只有A和B可以选，所以连接FA，FB，此为第2此构造。
- 然后插入了C点，同样地，C点只有A，B，F可选，连接CA，CB，CF，此为第3次构造。
- 重点来了，然后插入了E点，E点在A，B，F，C中只能选择3个点（m=3）作为“友点”，根据我们前面讲规则，要选最近的三个，怎么确定最近呢？朴素查找！从A，B，C，F任意一点出发，计算出发点与E的距离和出发点的所有“友点”和E的距离，选出最近的一点作为新的出发点，如果选出的点就是出发点本身，那么看我们的m等于几，如果不够数，就继续找第二近的点或者第三近的点，本着不找重复点的原则，直到找到3个近点为止。由此，我们找到了E的三个近点，连接EA，EC，EF，此为第四次构造。
- 第5次构造和第6次与E点的插入一模一样，都是在“现成”的图中查找到3个最近的节点作为“友点”，并做连接。

 图画完了，**请关注E点和A点的连线，如果我再这个图的基础上再插入6个点，这6个点有3个和E很近，有3个和A很近，那么距离E最近的3个点中没有A，距离A最近的3个点中也没有E，但因为A和E是构图早期添加的点，A和E有了连线，我们管这种连线叫“高速公路”，在查找时可以提高查找效率**（当进入点为E，待查找距离A很近时，我们可以通过AE连线从E直接到达A，而不是一小步一小步分多次跳转到A）

关于NSW算法的朴素构思就讲到这里了，下面我们来说说优化。

- 在查找的过程中，为了提高效率，我们**可以建立一个废弃列表，在一次查找任务中遍历过的点不再遍历。在一次查找中，已经计算过这个点的所有友点距离查找点的距离，并且已经知道正确的跳转方向了，这些结果是唯一的，没有必要再去做走这个路径，因为这个路径会带给我们同样的重复结果，没有意义。**

- 在查找过程中，为了提高准确度，**建立一个动态列表，把距离查找点最近的n个点存储在表中，并行地对这n个点进行同时计算“友点”和待查找点的距离，在这些“友点”中选择n个点与动态列中的n个点进行并集操作，在并集中选出n个最近的友点，更新动态列表**

#### HNSW

- **跳表结构**

设有有序链表，名叫sorted_link，里面有n个节点，每个节点是一个整数。我们从表头开始查找，查找第t（0<t<n）个节点，需要跳转几次？答：t-1次（没错，我是从1开始数的）。把n个节点分成n次查找的需求，都查找一遍，需要跳转几次？答：（0+1+2+3+.....+（n-1））次，如果我这链表长成下图这样呢？

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/27-3.png" style="zoom:50%;" />

这已经不是一个有序链表了，这是三个有序链表+分层连接指针构成的跳表了。看这张示意图就能明白它的查找过程，先查第一层，然后查第二层，然后查第三层，然后找到结果

**跳表怎么构建**呢？三个字，抛硬币。对于sorted_link链表中的每个节点进行抛硬币，如抛正，则该节点进入上一层有序链表，每**个sorted_link中的节点有50%的概率进入上一层有序链表。将上一层有序链表中和sorted_link链表中相同的元素做一一对应的指针链接**。再从sorted_link上一层链表中再抛硬币，sorted_link上一层链表中的节点有50%的可能进入最表层，相当于sorted_link中的每个节点有25%的概率进入最表层。以此类推。

这样就保证了**表层是“高速通道”，底层是精细查找，这个思想被应用到了NSW算法中，变成了其升级版-----HNSW**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/27-4.png" style="zoom:25%;" />

第0层中，是数据集中的所有点，你需要设置一个常数ml，通过公式**floor(-ln(uniform(0,1)) x ml)来计算这个点可以深入到第几层。公式中x是乘号，floor（）**的含义是向下取整，uniform（0,1）**的含义是在均匀分布中随机取出一个值，ln（）表示取对数**

关于HNSW算法的描述就基本结束了。我们来大致梳理一下它的**查找过程**：

- 从表层（上图中编号为Layer=2）任意点开始查找，选择进入点最邻近的一些友点，把它们存储在定长的动态列表中，别忘了把它们也同样在废弃表中存一份，以防后面走冤枉路。
- 一般地，在第x次查找时，先计算动态列表中所有点的友点距离待查找点（上图绿色点）的距离，在废弃列表中记录过的友点不要计算，计算完后更新废弃列表，不走冤枉路，再把这些计算完的友点存入动态列表，去重排序，保留前k个点，看看这k个点和更新前的k个点是不是一样的，如果不是一样的，继续查找，如果是一样的，返回前m个结果

**插入构图：先计算这个点可以深入到第几层，在每层的NSW图中查找t个最紧邻点，分别连接它们，对每层图都进行如此操作，描述完毕**

**需要控制一大堆参数：**

- 插入时的动态列表c的大小，它的大小直接影响了插入效率，和构图的质量，size越大，图的质量越高，构图和查找效率就越低
- 其次，一个节点至少有几个“友点”，“友点”越多，图的质量越高，查找效率越低。作者在论文中还提到了“max友点连接数”这个参数，设置一个节点至多有多少友点，来提高查找效率，但是设的太小又会影响图的质量，权衡着来。
- 上一段中的ml也是你来控制的，设置的大了，层数就少，内存消耗少，但严重影响效率，太大了会严重消耗内存和构图时间

#### 局部敏感哈希

#### 局部敏感哈希的基本原理

- **局部敏感哈希的基本思想是希望让相邻的点落入同一个“桶”，这样在进行最近邻搜索时，我们仅需要在一个桶内，或相邻几个桶内的元素中进行搜索即可**。如果保持每个桶中的元素个数在一个常数附近，我们就可以把最近邻搜索的时间复杂度降低到常数级别。那么，**如何构建局部敏感哈希中的“桶”呢**？下面，我们以基于欧式距离的最近邻搜索为例，来解释构建局部敏感哈希“桶”的过程。
- 首先，我们要弄清楚一个问题，**如果将高维空间中的点向低维空间进行映射，其欧式相对距离是不是会保持不变呢**？以图 4 为例，图 4 中间的彩色点处在二维空间中，当我们把二维空间中的点通过不同角度映射到 a、b、c 这三个一维空间时，可以看到原本相近的点，在一维空间中都保持着相近的距离。而原本远离的绿色点和红色点在一维空间 a 中处于接近的位置，却在空间 b 中处于远离的位置。
- 因此我们可以得出一个定性的结论：**欧式空间中，将高维空间的点映射到低维空间，原本接近的点在低维空间中肯定依然接近，但原本远离的点则有一定概率变成接近的点。**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/23-4.jpg" style="zoom:33%;" />

- **利用低维空间可以保留高维空间相近距离关系的性质，我们就可以构造局部敏感哈希“桶”**。
- 对于 Embedding 向量来说，**由于 Embedding 大量使用内积操作计算相似度，因此我们也可以用内积操作来构建局部敏感哈希桶。假设 v 是高维空间中的 k 维 Embedding 向量，x 是随机生成的 k 维映射向量。那我们利用内积操作可以将 v 映射到一维空间，得到数值 h(v)=v⋅x**。
- 而且，**一维空间也会部分保存高维空间的近似距离信息**。因此，**我们可以使用哈希函数 h(v) 进行分桶，公式为：h(v)=⌊(x⋅v+b)/w] 。其中， ⌊⌋ 是向下取整操作， w 是分桶宽度，b 是 0 到 w 间的一个均匀分布随机变量，避免分桶边界固化。**
- 不过，**映射操作会损失部分距离信息，如果我们仅采用一个哈希函数进行分桶，必然存在相近点误判的情况，因此，我们可以采用 m 个哈希函数同时进行分桶。如果两个点同时掉进了 m 个桶，那它们是相似点的概率将大大增加**。通过分桶找到相邻点的候选集合后，我们就可以在有限的候选集合中通过遍历找到目标点真正的 K 近邻了。
- 刚才讲的哈希策略是基于内积操作来制定的，内积相似度也是我们经常使用的相似度度量方法，事实上距离的定义有很多种，比如“曼哈顿距离”“切比雪夫距离”“汉明距离”等等。**针对不同的距离定义，分桶函数的定义也有所不同，但局部敏感哈希通过分桶方式保留部分距离信息，大规模降低近邻点候选集的思想是通用的**。

#### 局部敏感哈希的多桶策略

- 刚才讲到了可以使用多个分桶函数的方式来增加找到相似点的概率。**如果有多个分桶函数的话，具体应该如何处理不同桶之间的关系**呢？这就涉及局部敏感哈希的多桶策略。

- 假设有 A、B、C、D、E 五个点，有 h1和 h2两个分桶函数。使用 h1来分桶时，A 和 B 掉到了一个桶里，C、D、E 掉到了一个桶里；使用 h2来分桶时，A、C、D 掉到了一个桶里，B、E 在一个桶。那么请问如果我们想找点 C 的最近邻点，应该怎么利用两个分桶结果来计算呢

- 如果**用“且”（And）操作来处理两个分桶结果之间的关系，那么结果是这样的，找到与点 C 在 h1函数下同一个桶的点，且在 h2函数下同一个桶的点，作为最近邻候选点**。我们可以看到，满足条件的点只有一个，那就是点 D。也就是说，点 D 最有可能是点 C 的最近邻点。

- **如果采用“或”（Or）操作作为多桶策略，具体操作就是，我们找到与点 C 在 h1函数下同一个桶的点，或在 h2函数下同一个桶的点**。这个时候，我们可以看到候选集中会有三个点，分别是 A、D、E。这样一来，**虽然我们增大了候选集的规模，减少了漏掉最近邻点的可能性，但增大了后续计算的开销**。当然，局部敏感哈希的多桶策略还可以更加复杂，比如使用 3 个分桶函数分桶，把同时落入两个桶的点作为最近邻候选点等等。

- 我们应该选择“且”操作还是“或”操作，以及到底该选择使用几个分桶函数，每个分桶函数分几个桶呢？这些都还是工程上的权衡问题。一些取值的建议：
  - **点数越多，我们越应该增加每个分桶函数中桶的个数；相反，点数越少，我们越应该减少桶的个数**；
  - **Embedding 向量的维度越大，我们越应该增加哈希函数的数量，尽量采用且的方式作为多桶策略；相反，Embedding 向量维度越小，我们越应该减少哈希函数的数量，多采用或的方式作为分桶策略**。
- 最后，局部敏感哈希能在常数时间得到最近邻的结果吗？答案是可以的，**如果我们能够精确地控制每个桶内的点的规模是 C，假设每个 Embedding 的维度是 N，那么找到最近邻点的时间开销将永远在 O(C⋅N) 量级。采用多桶策略之后，假设分桶函数数量是 K，那么时间开销也在 O(K⋅C⋅N) 量级，这仍然是一个常数**。

#### 局部敏感哈希实践

- 利用 Sparrow Recsys 训练好的物品 Embedding，来实现局部敏感哈希的快速搜索。为了保证跟 Embedding 部分的平台统一，**使用 Spark MLlib 完成 LSH 的实现**。
- 在将电影 Embedding 数据转换成 dense Vector 的形式之后，我们**使用 Spark MLlib 自带的 LSH 分桶模型 BucketedRandomProjectionLSH（我们简称 LSH 模型）来进行 LSH 分桶**。其中最关键的部分是设定 LSH 模型中的 BucketLength 和 NumHashTables 这两个参数。其中，**BucketLength 指的就是分桶公式中的分桶宽度 w，NumHashTables 指的是多桶策略中的分桶次数**。清楚了模型中的关键参数，执行的过程就跟我们讲过的其他 Spark MLlib 模型一样了，都是**先调用 fit 函数训练模型，再调用 transform 函数完成分桶的过程**，具体的实现可以参考下面的代码

```scala
def embeddingLSH(spark:SparkSession, movieEmbMap:Map[String, Array[Float]]): Unit ={
  //将电影embedding数据转换成dense Vector的形式，便于之后处理
  val movieEmbSeq = movieEmbMap.toSeq.map(item => (item._1, Vectors.dense(item._2.map(f => f.toDouble))))
  val movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF("movieId", "emb")

  //利用Spark MLlib创建LSH分桶模型
  val bucketProjectionLSH = new BucketedRandomProjectionLSH()
    .setBucketLength(0.1)
    .setNumHashTables(3)
    .setInputCol("emb")
    .setOutputCol("bucketId")
  //训练LSH分桶模型
  val bucketModel = bucketProjectionLSH.fit(movieEmbDF)
  //进行分桶
  val embBucketResult = bucketModel.transform(movieEmbDF)
  
  //打印分桶结果
  println("movieId, emb, bucketId schema:")
  embBucketResult.printSchema()
  println("movieId, emb, bucketId data result:")
  embBucketResult.show(10, truncate = false)
  
  //尝试对一个示例Embedding查找最近邻
  println("Approximately searching for 5 nearest neighbors of the sample embedding:")
  val sampleEmb = Vectors.dense(0.795,0.583,1.120,0.850,0.174,-0.839,-0.0633,0.249,0.673,-0.237)
  bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate = false)
}
```

- 使用 LSH 模型对电影 Embedding 进行分桶得到的五个结果打印了出来，如下所示：

```
+-------+-----------------------------+------------------+
|movieId|emb                          |bucketId          |
+-------+-----------------------------+------------------------+
|710    |[0.04211471602320671,..]     |[[-2.0], [14.0], [8.0]] |
|205    |[0.6645985841751099,...]     |[[-4.0], [3.0], [5.0]]  |
|45     |[0.4899883568286896,...]     |[[-6.0], [-1.0], [2.0]] |
|515    |[0.6064003705978394,...]     |[[-3.0], [-1.0], [2.0]] |
|574    |[0.5780771970748901,...]     |[[-5.0], [2.0], [0.0]]  |
+-------+-----------------------------+------------------------+
```

- 在 BucketId 这一列，因为我们之前设置了 **NumHashTables 参数为 3，所以每一个 Embedding 对应了 3 个 BucketId。在实际的最近邻搜索过程中，我们就可以利用刚才讲的多桶策略进行搜索了**