### 1. 前言

- 本文总结目标检测常见的评价指标，例如：**Acc、Recall、Precision、IoU、AP、mAP**等概念。

### 2. 常见指标总结

#### 2.1 准确率 (Accuracy)

- **分对的样本数除以所有的样本数 ，即：准确（分类）率 = 正确预测的正反例数 / 总数。**
- 准确率一般用来评估模型的全局准确程度，不能包含太多信息，无法全面评价一个模型性能。

#### 2.2 混淆矩阵 (Confusion Matrix)

- 混淆矩阵中的**横轴是模型预测的类别数量统计，纵轴是数据真实标签的数量统计。**
- **对角线，表示模型预测和数据标签一致的数目，**所以对角线之和除以测试集总数就是**准确率**。对角线上数字越大越好，在可视化结果中颜色越深，说明模型在该类的预测准确率越高。如果按行来看，每行不在对角线位置的就是错误预测的类别。总的来说，我们希望对角线越高越好，非对角线越低越好。

#### 2.3 精确率(Precision)与召回率(Recall)

- 先得了解**TP、TN、FP、FN**的概念
- 假设现在有这样一个测试集，**测试集中的图片只由大雁和飞机两种图片**组成，假设你的分类系统**最终的目的**是：能取出测试集中所有飞机的图片，而不是大雁的图片。**飞机为正样本，大雁为负样本**。

- True positives : **正样本被正确识别为正样本**，飞机的图片被正确的识别成了飞机。 
- True negatives: **负样本被正确识别为负样本**，大雁的图片没有被识别出来，系统正确地认为它们是大雁

- False positives: **假的正样本，即负样本被错误识别为正样本**，大雁的图片被错误地识别成了飞机
- False negatives: **假的负样本，即正样本被错误识别为负样本**，飞机的图片没有被识别出来，系统错误地认为它们是大雁。

- **Precision**其实就是**在识别出来的图片中，True positives所占的比率。**也就是本假设中，所有被识别出来的飞机中，真正的飞机所占的比例。

$$
\text { precision }=\frac{t p}{t p+f p}=\frac{t p}{n}
$$

- 两种极端情况就是，**如果精度是100%，就代表所有分类器分出来的正类确实都是正类。如果精度是0%，就代表分类器分出来的正类没一个是正类。**
- 光是精确度还不能衡量分类器的好坏程度，**比如50个正样本和50个负样本，我的分类器把49个正样本和50个负样本都分为负样本，剩下一个正样本分为正样本，这样我的精度也是100%，因为分类器分出来1个样本为正类，且这个样本分类正确，则1/1 = 1**。所以还需要召回率
- **Recall 是测试集中所有正样本样例中，被正确识别为正样本的比例**。也就是本假设中，被正确识别出来的飞机个数与测试集中所有真实飞机的个数的比值。

$$
\text { precision }=\frac{t p}{t p+f n}
$$

- 注：**目标检测中：**为了获得True Positives and False Positives，我们需要使用IoU（在下文中介绍如何计算）。计算IoU，我们从而确定一个检测结果（Positive）是正确的（True）还是错误的（False）。
- **最常用的阈值是0.5，即如果IoU> 0.5，则认为它是True Positive，否则认为是False Positive。而COCO数据集的评估指标建议对不同的IoU阈值进行计算，但为简单起见，我们这里仅讨论一个阈值0.5，这是PASCAL VOC数据集所用的指标。**

#### 2.4  Precision-recall 曲线

- 改变识别阈值，使得**系统依次能够识别前K张图片，阈值的变化同时会导致Precision与Recall值发生变化**，从而得到曲线。

- 如果一个分类器的性能比较好，那么它应该有如下的表现：**在Recall值增长的同时，Precision的值保持在一个很高的水平**。而性能比较差的分类器可能会损失很多Precision值才能换来Recall值的提高。通常情况下，文章中都会使用Precision-recall曲线，来显示出分类器在Precision与Recall之间的权衡。

#### 2.5 F1 score

- 有时候**使用两个指标不太好评价模型之间的好坏，因为可能出现模型A的精确度比模型B高，但是A的召回率比模型B低。**
- 解决该问题的办法就是**结合精确度和召回率计算得到另外一个指标：F1 score**，计算公式如下，P表示precision，R表示recall：

$$
\mathrm{F} 1=\frac{2 * P * R}{P+R}
$$

#### 2.6 IoU

- 在目标检测算法中，我们经常需要评价2个矩形框之间的相似性，直观来看可以通过比较2个框的距离、重叠面积等计算得到相似性，而IoU指标恰好可以实现这样的度量。
- **IoU（intersection over union，交并比）是目标检测算法中用来评价2个矩形框之间相似度的指标**
- **IoU = 两个矩形框相交的面积 / 两个矩形框相并的面积**，如下图所示：

#### 2.7 AP

- PR曲线下的面积就定义为AP，即：

$$
A P=\int_{0}^{1} p(r) d r
$$

- 实际计算过程中，PASCAL VOC，COCO比赛在上述基础上都有不同的调整策略。
- **Interpolated AP（PASCAL VOC 2008的评测指标）**

- 在PASCAL VOC 2008中，在计算AP之前会对上述曲线进行平滑，平滑方法为，对每一个Precision值，使用其右边最大的Precision值替代。具体示意图如下：

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/500%E9%97%AE/5-1.jpg" style="zoom:50%;" />

- 通过**平滑策略，上面蓝色的PR曲线就变成了红色的虚线**了。平滑的好处在于，平滑后的曲线单调递减，不会出现摇摆的情况。这样的话，随着Recall的增大，Precision逐渐降低，这才是符合逻辑的。
- **实际计算时，对平滑后的Precision曲线进行均匀采样出11个点（每个点间隔0.1），然后计算这11个点的平均Precision**。具体如下：

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/500%E9%97%AE/5-2.jpg" style="zoom:50%;" />

- **AP (Area under curve AUC，PASCAL VOC2010–2012评测指标)**
- 上述11点插值的办法由于插值点数过少，容易导致结果不准。一个**解决办法就是内插所有点。所谓内插所有点，其实就是对上述平滑之后的曲线算曲线下面积**。
- 这样计算之所以会更准确一点，可以这么看！原先11点采样其实算的是曲线下面积的近似，具体近似办法是：取10个宽为0.1，高为Precision的小矩形的面积平均。现在这个则不然，现在这个算了无数个点的面积平均，所以结果要准确一些。示意图如下：

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/500%E9%97%AE/5-3.jpg" style="zoom:50%;" />

#### 2.8 COCO mAP

- 对于各个类别，分别按照上述方式计算AP，**取所有类别的AP平均值就是mAP。**这就是在目标检测问题中mAP的计算方法。
- 最新的目标检测相关论文都使用coco数据集来展示自己模型的效果。
- **对于coco数据集来说，使用的也是Interplolated AP的计算方式。与Voc 2008不同的是，为了提高精度，在PR曲线上采样了100个点进行计算。而且Iou的阈值从固定的0.5调整为在 0.5 - 0.95 的区间上每隔0.5计算一次AP的值，取所有结果的平均值作为最终的结果。**

- 通常来说**AP是在单个类别下的，mAP是AP值在所有类别下的均值**。在这里，在coco的语境下AP便是mAP，这里的AP已经计算了所有类别下的平均值，这里的AP便是mAP。

