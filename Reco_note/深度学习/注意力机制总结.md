#### 1. SE-A

- **论文原文部分如下：**

![SE](/Users/yearing1017/Desktop/论文图/SE.png)

> The features U are ﬁrst passed through a **squeeze operation**, which produces a channel descriptor by aggregating feature maps across their spatial dimensions (H × W ). The function of this descriptor is to produce an embedding of the global distribution of channel-wise feature responses, allowing information from the global receptive ﬁeld of the network to be used by all its layers.

> The aggregation is followed by an **excitation operation**, which takes the form of a simple self-gating mechanism that takes the embedding as input and produces a collection of per-channel modulation weights. These weights are applied to the feature maps U to generate the output of the SE block which can be fed directly into subsequent layers of the network.

- **squeeze部分介绍**

![image-20201018093725064](/Users/yearing1017/Library/Application Support/typora-user-images/image-20201018093725064.png)

- **Excitation操作及最后的特征重定向**

![image-20201023214227145](/Users/yearing1017/Library/Application Support/typora-user-images/image-20201023214227145.png)

- 注意力模块由 3 部分组成：挤压层（squeeze）、激励层（excitation）和注意层（attention）。 挤压层是一 个 全 局 最 大 池 化 ，负 责 将 二 维 的 高 级 特 征 （ H × W × C ）压缩为一维特征（1 × 1 × C ）。 激励层由 1 × 1 × C/16 的 卷 积 、relu、1 × 1 × C 的 卷 积 和 一 个 sigmoid 层组成。 前面的两层卷积用于学习不同通道之间的关联性，相较于直接使用一个卷积层，这样做有两点好处：1）卷积-relu-卷积的结构具有更多的 非线性，可以更好地拟合通道间复杂的相关性；2）降维-升维的方式可以减少参数量和计算量。 sigmoid 层则负责将学习到的权重压缩到 0-1 之间。 最后注意层将归一化的权重加权到低级特征上。

