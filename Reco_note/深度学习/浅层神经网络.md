## **1. Neural Networks Overview**

首先，我们从整体结构上来大致看一下神经网络模型。

在Ng前面的课程中，我们已经使用计算图的方式介绍了逻辑回归梯度下降算法的正向传播和反向传播两个过程。如下图所示。神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为隐藏层或中间层。这样从计算上来说，神经网络的正向传播和反向传播过程只是比逻辑回归多了一次重复的计算。

正向传播过程分成两层，第一层是输入层到隐藏层，用上标[1]来表示：
$$
z^{[1]}=w^{[1]}x+b
$$

$$
\alpha^{[1]}=\sigma(z^{[1]})
$$

第二层是隐藏层到输出层，用上标[2]来表示：
$$
z^{[2]}=w^{[2]}\alpha^{[1]}+b^{[2]}
$$

$$
\alpha^{[2]}=\sigma(z^{[2]})
$$

其中，方括号上标[i]表示当前所处的层数；圆括号上标(i)表示第i个样本。

同样，反向传播过程也分成两层。第一层是输出层到隐藏层，第二层是隐藏层到输入层。

下图为逻辑回归与神经网络的示意图：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89/5-1.jpg)

## **2. Neural Network Representation**

下面我们以图示的方式来介绍单隐藏层的神经网络结构。如下图所示，单隐藏层神经网络就是典型的浅层（shallow）神经网络。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89/5-2.jpg)

结构上，从左到右，可以分成三层：输入层（Input layer），隐藏层（Hidden layer）和输出层（Output layer）。输入层和输出层，顾名思义，对应着训练样本的输入和输出，很好理解。隐藏层是抽象的非线性的中间层，这也是其被命名为隐藏层的原因。

在写法上，我们通常把输入矩阵X记为$\alpha^{[0]}$，把隐藏层输出记为$\alpha^{[1]}$，上标从0开始。用下标表示第几个神经元，注意下标从1开始。例如$\alpha_1^{[1]}$表示隐藏层第1个神经元，$\alpha_2^{[1]}$表示隐藏层第2个神经元，等等。这样，隐藏层有4个神经元就可以将其输出$a^{[1]}$写成矩阵的形式：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89/5-3.jpg)

最后，相应的输出层记为$a^{[2]}$，即$\hat y$。这种单隐藏层神经网络也被称为两层神经网络（2 layer NN）。之所以叫两层神经网络是因为，通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为0的原因$（a^{[0]}）$。

## **3. Computing a Neural Network’s Output**

接下来我们开始详细推导神经网络的计算过程。回顾一下，我们前面讲过两层神经网络可以看成是逻辑回归再重复计算一次。如下图所示，逻辑回归的正向计算可以分解成计算z和a的两部分：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89/5-4.jpg)

对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要注意对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如$\alpha_i^{[l]}$表示第l层的第i个神经元。注意，i从1开始，l从0开始。

下面，我们将从输入层到隐藏层的计算公式列出来：
$$
z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]},a_1^{[1]}=\sigma(z_1^{[1]})
$$

$$
z_2^{[1]}=w_2^{[1]T}x+b_2^{[1]},a_2^{[1]}=\sigma(z_2^{[1]})
$$

$$
z_3^{[1]}=w_3^{[1]T}x+b_3^{[1]},a_3^{[1]}=\sigma(z_3^{[1]})
$$

$$
z_4^{[1]}=w_4^{[1]T}x+b_4^{[1]},a_4^{[1]}=\sigma(z_4^{[1]})
$$

然后，从隐藏层到输出层的计算公式为：
$$
z_1^{[2]}=w_1^{[2]T}a^{[1]}+b_1^{[2]},a_1^{[2]}=\sigma(z_1^{[2]})
$$
其中$a^{[1]} $为：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89/5-3.jpg)

上述每个节点的计算都对应着一次逻辑运算的过程，分别由计算z和a两部分组成。

为了提高程序运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89/5-5.jpg)

$W^{[1]}$的维度是（4,3），$b^{[1]}$的维度是（4,1），$W^{[2]}$的维度是（1,4），$b^{[2]}$的维度是（1,1）。这点需要特别注意。

