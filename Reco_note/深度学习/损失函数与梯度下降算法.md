### 1、算法的模型表示
首先回顾一下之前的房价预测问题，我们现在拥有某一个城市的住房价格数据，基于数据，可绘制如下所示图像，在已有的房价数据中，已知一些房子的大小和价格，现在我们知道一个房子的大小是1250平方英尺，如何基于房价数据去预测该房子的售价？
![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-1.jpg)
如果用一条直线去拟合数据，那么你的房子可能的售价是$220,000。为了更好的描述模型，我们根据其数据集的格式定义一些符号。
![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2.jpg)

- m表示训练集中实例的数量
- x表示输入变量（或者说是特征），这里是房间的大小
- y表示输出变量（或者说目标值），这里是房价
- (x, y)表示一条训练样例，$(x^i,y^i)$表示第i个样本例，如（2104， 460）是第一条记录

监督学习的过程：
![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-3.jpg)

如上所示，我们将训练的数据集丢给（feed）学习算法，学习算法输出一个函数h（这里的h其实就是hypothesis假设），h的任务就是给定一个输入x，然手输出房子的估价y。所以，h是一个从x映射到y的函数，有的人会问为什么函数h会被成为假设，ng表示说这只是机器学习的标准术语，不必要纠结。但是我们如何来表示函数h呢？根据房价预测问题，我们可以用一种直线去拟合数据（如第一张图），因此在这里，h函数的一种可能方式就是:
$$
h_\Theta(x) = \Theta_0+\Theta_1(x)
$$
![2](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-4.jpg)
这里的函数h其实就是我们的模型表达方式，因为只有一个变量x，所以称为单变量线性回归问题。

### 2、损失函数

上一块内容中总结的预测房价的模型表达方式（函数h）$h_\theta(x) = \theta_0+\theta_1(x）  ​$，$\theta_0​$和$\theta_1 ​$是模型参数，若分别取其值为1.5、0，0、0.5，1、0.5，所对应的函数h图像如下所示：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-1.jpg)

在线性回归中，我们有一个训练数据集，然后我们可以用一条直线去拟合这些数据集（如上图表示），因此我们要做的就是通过训练集数据来学习出模型参数$\theta_0$和$\theta_1$，只有模型参数适合，才能使得函数h更好的拟合数据，使得$h_\theta(x)​$接近或等于训练样例（x,y）中的y值。

这样线性回归问题就转化成了，如何解决一个最小化问题。我们选择的模型参数$\theta_0 ​$和$\theta_1​$决定了我们的直线（模型）相对于数据集的准确程度，即模型所预测的值与真实值之间的差距。例如，取模型参数为0、0.5时，预测值与真实值的差距如下图所示：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-2.jpg)

上图中，黑色直线是选择模型参数为0、0.5的函数h，淡红色叉是真实值，蓝色直线是真实值与预测值之间的差距，我们的目标就是选择出使得模型的误差最小的模型参数，一般我们使用误差的平方和来表示，所以其计算公式如下：





$$
J(\theta_0,\theta_1)=\frac{1}{2m}\sum_1^m(h_\Theta(x^i)-y^i)^2
$$
这里的$J(\theta_0,\theta_1) $就是损失函数。简单点来理解，损失函数就是给定的模型参数其模型预测的值与真实值出现误差的代价。这个时候如果我们根据刚刚提到的房价数据来绘制一个等高线图，三个坐标分别是参数$\theta _0$,$\theta _1$和损失函数$J(\theta_0$,$\theta_1$)，如下图： 

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-3.png)

如果我们把这个等高线图只投影到坐标$\theta _0,\theta _1​$上去，可以得出下面的图：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-4.png)

根据上面的几张图，我们可以看出，存在一个可以使得损失函数$J(\theta_0,\theta_1)​$最小的参数点$\theta _0​$,$\theta _1 ​$

### 3、梯度下降算法

​	上部分定义了损失函数，而我们的目标就是最小化损失函数。梯度下降算法可以用来求解损失函数最小化。

这里如下介绍梯度下降算法。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-5.png)

上图介绍为：现在我们有有损失函数$J(\theta_0,\theta_1) ​$，目标是最小化该目标函数$J(\theta_0,\theta_1)​$。
梯度下降算法的一般步骤为：

- 初始化模型参数$\theta_0,\theta_1​$（可将其全部设为0或其他数值）
- 一点点改变$\theta_0,\theta_1$的数值，让损失函数$J(\theta_0,\theta_1)$变小，直到找到损失函数$J(\theta_0,\theta_1)$的最小值

通过三维图观察过程如下：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-6.png)

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-7.png)

​	上图中，图形的表面高度是损失函数的值，当然我们希望使得损失函数的值最小，这个时候我们从某一个参数点出发（出发点就是刚开始我们初始化参数的数值）。

​	NG在这里将上面的图形想象为一座山，并假设你当前站在出发点的位置，为了尽快的下山，我们应该朝着上面方向怎么走？如果我们站在山坡上的某一点，你看一下周围 ，你会发现最佳的下山方向大约是那个方向。好的，现在你在山上的新起点上，你再看看周围，然后再一次想想我应该从什么方向迈着小碎步下山? 然后你按照自己的判断又迈出一步，往那个方向走了一步。然后重复上面的步骤，从这个新的点，你环顾四周并决定从什么方向将会最快下山，然后又迈进了一小步，又是一小步…并依此类推，直到你接近局部最低点的位置。**（数学中，梯度其实是微积分的一个概念，在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的下降（上升）最快的方向，不明白的可以看看微积分相关知识）**。

​	但是我们发现，如上面两个图，当我们身处不同的起点的时候，通过梯度下降算法会得到不同的解，这是梯度下降算法的一个特点。

下图给出梯度下降算法的求解过程：

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-8.png)

这里的$\alpha$是学习速率，对比NG的想象下山中你下山走的步伐（是小碎步还是大跨步），$\alpha \frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$就是参数的梯度方向，对比NG的想象下山中你在当前位置选择下山的方向。另外，在梯度下降算法中，我们需要同时更新参数$\theta_0 $和$\theta_1 $，而不是交替更新。

下图解释梯度下降能够在梯度方向最小化损失函数和学习速率的取值的影响:

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-10.png)

根据损失函数$J(\theta_0,\theta_1)$（其实就是一个二次方程），能够画出上图随着参数$\theta_1$的变化，损失函数的变化。梯度的方向其实就是曲线的切线方向。而这个时候学习速率$ \alpha ​$过小，达到最小值点需要很多步；而学习速率过大，那么有可能会越过最小值点，甚至会远离最小值点，这样无法收敛。

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/yearing1017/1-2-11.png)

而如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做（求导为0，切线斜率为0），它不会改变参数的值，这也正是你想要的，因为它使你的解始终保持在局部最优点，这也解释了为什么即使学习速率 $ \alpha $保持不变时，梯度下降也可以收敛到局部最低点。