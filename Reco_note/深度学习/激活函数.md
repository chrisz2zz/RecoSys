####  常用激活函数及导数

**Sigmoid**激活函数的形式为：
$$
f(z)=\frac{1}{1+\exp (-z)}
$$
导数：
$$
f^{\prime}(z)=f(z)(1-f(z))
$$
**Tanh**激活函数的形式为：
$$
f(z)=\tanh (z)=\frac{\mathrm{e}^{z}-\mathrm{e}^{-z}}{\mathrm{e}^{z}+\mathrm{e}^{-z}}
$$
导数：
$$
f^{\prime}(z)=1-(f(z))^{2}
$$
**ReLU**激活函数的形式为：
$$
f(z) = max(0,z)
$$
导数：
$$
f^{\prime}(z)=\left\{\begin{array}{l}1, z>0 \\ 0, z \leqslant 0\end{array}\right.
$$

#### 为什么Sigmoid和Tanh激活函数会导致梯度消失的现象

- Sigmoid激活函数的曲线如图所示。它将输入z映射到区间（0，1），**当z很 大时，f(z)趋近于1；当z很小时，f(z)趋近于0。其导数 $f^{\prime}(z)=f(z)(1-f(z))$ 在z很大或很 小时都会趋近于0，造成梯度消失的现象**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%2021-1.png" style="zoom: 33%;" />

- Tanh激活函数的曲线如图9.8所示。**当z很大时，f(z)趋近于1；当z很小时，f(z) 趋近于−1。其导数 $f^{\prime}(z)=1-(f(z))^{2}$ 在z很大或很小时都会趋近于0，同样会出现“梯度消失”**。实际上，Tanh激活函数相当于Sigmoid的平移。

$$
tanh(x) = 2sigmoid(2x) - 1
$$

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-2.png" style="zoom:50%;" />

#### ReLU系列的激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？

**ReLU激活函数的优点**

- 从计算的角度上，Sigmoid和Tanh激活函数均需要计算指数，复杂度高，而**ReLU只需要一个阈值即可得到激活值** 
- **ReLU的非饱和性可以有效地解决梯度消失的问题**，提供相对宽的激活边界 
- ReLU的**单侧抑制提供了网络的稀疏表达能力**

**ReLU激活函数的局限性**

- ReLU的局限性在于其训练过程中会导致神经元死亡的问题。这是由于函数$f(z) = max(0,z)$导致**负梯度在经过该ReLU单元时被置为0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为0，不对任何数据产生响应。**

- 在实际训练中，**如果学习率（Learning Rate）设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败**。

**Leaky ReLU**

为解决这一问题，人们设计了ReLU的变种Leaky ReLU（LReLU），其形式表示为:
$$
f(z)=\left\{\begin{array}{l}z, z>0 \\ a z, z \leqslant 0\end{array}\right.
$$
ReLU和LReLU的函数曲线对比如图所示。**LReLU与ReLU的区别在于， 当z<0时其值不为0，而是一个斜率为a的线性函数**，一般a为一个很小的正常数， 这样**既实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失**。但另一方 面，a值的选择增加了问题难度，需要较强的人工先验或多次重复训练以确定合适的参数值

![](https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/21-3.png)

