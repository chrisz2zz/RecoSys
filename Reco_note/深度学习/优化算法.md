### 优化算法

#### 梯度下降和随机梯度下降的比较

**梯度下降容易收敛到局部最优**，所以基本不使用梯度下降来训练神经网络

**随机梯度下降容易逃离鞍点和泛化不好的最优点**，所以随机梯度下降和他的变种Adam等是训练神经网络最流行的方法

#### 经典梯度下降法存在的问题

机器学习中，优化问题的目标函数通常可以表示为：
$$
L(\theta)=\mathbb{E}_{(x, y) \sim P_{\text {data }}} L(f(x, \theta), y)
$$
其中θ是待优化的模型参数，x是模型输入，f(x,θ)是模型的实际输出，y是模型的目标输出，函数L刻画了模型在数据(x,y)上的损失，P data 表示数据的分布，E表示期望。因此，**L(θ)刻画了当参数为θ时，模型在所有数据上的平均损失。** 

**找到平均损失最小的模型参数，也就是求解优化问题：**
$$
\theta^{*}=\arg \min L(\theta)
$$
经典的**梯度下降法采用所有训练数据的平均损失来近似目标函数**，即：
$$
L(\theta)=\frac{1}{M} \sum_{i=1}^{M} L\left(f\left(x_{i}, \theta\right), y_{i}\right)
$$
其中M是训练样本的个数。模型参数的更新公式为：
$$
\nabla L(\theta)=\frac{1}{M} \sum_{i=1}^{M} \nabla L\left(f\left(x_{i}, \theta\right), y_{i}\right)
$$

$$
\theta_{t+1}=\theta_{t}-\alpha \nabla L\left(\theta_{t}\right)
$$

因此，**经典的梯度下降法在每次对模型参数进行更新时，需要遍历所有的训练数据。当M很大时，这需要很大的计算量，耗费很长的计算时间，在实际应用中基本不可行。**

#### 随机梯度下降

为了解决该问题，**随机梯度下降法（Stochastic Gradient Descent，SGD）用单个训练样本的损失来近似平均损失**，即
$$
\begin{array}{l}L\left(\theta ; x_{i}, y_{i}\right)=L\left(f\left(x_{i}, \theta\right), y_{i}\right) \\ \nabla L\left(\theta ; x_{i}, y_{i}\right)=\nabla L\left(f\left(x_{i}, \theta\right), y_{i}\right)\end{array}
$$
因此，**随机梯度下降法用单个训练数据即可对模型参数进行一次更新，大大加快了收敛速率。该方法也非常适用于数据源源不断到来的在线更新场景。**

#### 小批量梯度下降法

为了降低随机梯度的方差，从而使得迭代算法更加稳定，也为了充分利用高度优化的矩阵运算操作，在实际应用中我们会**同时处理若干训练数据，该方法被称为小批量梯度下降法（Mini-Batch Gradient Descent）**

假设需要同时处理m个训练数据${\{(x_{i1},y_{i1})...(x_{im},y_{im})\}}$ ，则目标函数及其梯度为：
$$
\begin{array}{l}L(\theta)=\frac{1}{m} \sum_{j=1}^{m} L\left(f\left(x_{i_{j}}, \theta\right), y_{i_{j}}\right) \\ \nabla L(\theta)=\frac{1}{m} \sum_{j=1}^{m} \nabla L\left(f\left(x_{i_{j}}, \theta\right), y_{i_{j}}\right)\end{array}
$$
参数的设定

- **如何选取参数m**？在不同的应用中，最优的m通常会不一样，需要通过调参选取。一般**m取2的幂次时能充分利用矩阵运算操作**，所以可以在2的幂次中挑选最优的取值，例如32、64、128、256等
- **如何挑选m个训练数据**？为了避免数据的特定顺序给算法收敛带来的影响，一般会在每次遍历训练数据之前，**先对所有的数据进行随机排序，然后在每次迭代时按顺序挑选m个训练数据直至遍历完所有的数据**
- **如何选取学习速率α？**为了加快收敛速率，同时提高求解精度，通常会采用**衰减学习速率的方案：一开始算法采用较大的学习速率，当误差曲线进入平台期后，减小学习速率做更精细的调整**。最优的学习速率方案也通常需要调参才能得到

综上，通常采用小批量梯度下降法解决训练数据量过大的问题。每次更新模型参数时，只需要处理m个训练数据即可，其中m是一个远小于总数据量M的常数，这样能够大大加快训练过程。

#### 随机梯度下降法失效

深度学习中最常用的优化方法是随机梯度下降法，但是随机梯度下降法偶尔也会失效，无法给出满意的训练结果，这是为什么？

**一个形象的比喻**。想象一下，你正在下山，视力很好，能看清自己所处位置的坡度，那么沿着坡向下走，最终你会走到山底。 如果你被蒙上双眼，只能凭脚底踩石头的感觉判断当前位置的坡度，精确性就大 大下降，有时候你认为的坡，实际上可能并不是坡，走上一段时间发现没有下山，或者曲曲折折走了好多弯路才下山

类似地，**批量梯度下降法（Batch Gradient Descent，BGD）就好比正常下山， 而随机梯度下降法就好比蒙着眼睛下山**

批量梯度下降法的每一步都把整个训练集载入进来进行计算，时间花费和内存开销都非常大，无法应用于大数据集、大模型的场景。相反，随机梯度下降法则放弃了对梯度准确性的追求，**每步仅仅随机采样一个（或少量）样本来估计当前梯度，计算速度快**，内存开销小。但由于每步接受的信息量有限，**随机梯度下降法对梯度的估计常常出现偏差，造成目标函数曲线收敛得很不稳定，伴有剧烈波动，有时甚至出现不收敛的情况**

对随机梯度下降法来说，**可怕的不是局部最优点，而是山谷和鞍点两类地形**

山谷顾名思义就是狭长的山间小道，左右两边是峭壁；鞍点的形状像是一个马鞍，一个方向上两头翘，另一个方向上两头垂，而中心区域是一片近乎水平的平地。

为什么随机梯度下降法最害怕遇上这两类地形呢？

- 在山谷中，准确的梯度方向是沿山道向下，稍有偏离就会撞向山壁，而**粗糙的梯度估计使得它在两山壁间来回反弹震荡，不能沿山道方向迅速下降，导致收敛不稳定和收敛速度慢**
- 在**鞍点处，随机梯度下降法会走入一片平坦之地**（此时离最低点还很远，故也称 plateau）。想象一下蒙着双眼只凭借脚底感觉坡度，如果坡度很明显，那么基本能估计出下山的大致方向；如果坡度不明显，则很可能走错方向。**同样，在梯度近乎为零的区域，随机梯度下降法无法准确察觉出梯度的微小变化，结果就停滞下来**

#### 解决方法：惯性和环境感知

#### 动量方法

**随机梯度下降法本质上是采用迭代方式更新参数，每次迭代在当前位置的基础上，沿着某一方向迈一小步抵达下一位置，然后在下一位置重复上述步骤**。随机梯度下降法的更新公式表示为
$$
\theta_{t+1}=\theta_{t}-\eta g_{t}
$$
其中，**当前估计的负梯度$-g_t$表示步子的方向，学习速率η控制步幅**。改造的随机梯度下降法仍然基于这个更新公式

为了解决**随机梯度下降法山谷震荡和鞍点停滞**的问题，我们做一个简单的思维实验。想象一下**纸团在山谷和鞍点处的运动轨迹**，在山谷中纸团受重力作用沿山道滚下，两边是不规则的山壁，纸团不可避免地撞在山壁，由于质量小受山壁弹力的干扰大，从一侧山壁反弹回来撞向另一侧山壁，结果来回震荡地滚下；如果当纸团来到鞍点的一片平坦之地时，还是由于质量小，速度很快减为零。

**纸团的情况和随机梯度下降法遇到的问题简直如出一辙**。直观地，**如果换成一个铁球，当沿山谷滚下时，不容易受到途中旁力的干扰，轨迹会更稳更直；当来到鞍点中心处，在惯性作用下继续前行，从而有机会冲出这片平坦的陷阱**。

因此，有了**动量方法，模型参数的迭代公式为**:
$$
\begin{array}{c}v_{t}=\gamma v_{t-1}+\eta g_{t} \\ \theta_{t+1}=\theta_{t}-v_{t}\end{array}
$$
具体来说，**前进步伐$-v_t$由两部分组成。一是学习速率η乘以当前估计的梯度$g_t$ ；二 是带衰减的前一次步伐$v_{t-1}$**

这里，惯性就体现在对前一次步伐信息的重利用上。 类比中学物理知识，**当前梯度就好比当前时刻受力产生的加速度，前一次步伐好比前一时刻的速度，当前步伐好比当前时刻的速度**。为了计算当前时刻的速度， 应当考虑前一时刻速度和当前加速度共同作用的结果，因此$v_t$ 直接依赖于$v_{t-1}$ 和$g_t$ ， 而不仅仅是$g_t$ 。另外，衰减系数γ扮演了阻力的作用

中学物理还告诉我们，**刻画惯性的物理量是动量，这也是算法名字的由来**。 沿山谷滚下的铁球，会受到沿坡道向下的力和与左右山壁碰撞的弹力。向下的力稳定不变，产生的动量不断累积，速度越来越快；左右的弹力总是在不停切换， 动量累积的结果是相互抵消，自然减弱了球的来回震荡。因此，**与随机梯度下降法相比，动量方法的收敛速度更快，收敛曲线也更稳定**

#### AdaGrad方法

惯性的获得是基于历史信息的，那么，除了从过去的步伐中获得一股子向前冲的劲儿，还能获得什么呢？**我们还期待获得对周围环境的感知，即使蒙上双眼，依靠前几次迈步的感觉，也应该能判断出一些信息，比如这个方向总是坑坑洼洼的，那个方向可能很平坦**

随机梯度下降法**对环境的感知是指在参数空间中，根据不同参数的一些经验性判断，自适应地确定参数的学习速率，不同参数的更新步幅是不同的**。例如， 在文本处理中训练词嵌入模型的参数时，有的词或词组频繁出现，有的词或词组则极少出现。数据的稀疏性导致相应参数的梯度的稀疏性，不频繁出现的词或词组的参数的梯度在大多数情况下为零，从而这些参数被更新的频率很低。**在应用中，我们希望更新频率低的参数可以拥有较大的更新步幅，而更新频率高的参数的步幅可以减小**

**AdaGrad方法采用“历史梯度平方和”来衡量不同参数的梯度的稀疏性，取值越小表明越稀疏**，具体的更新公式表示为
$$
\theta_{t+1, i}=\theta_{t, i}-\frac{\eta}{\sqrt{\sum_{k=0}^{t} g_{k, i}^{2}+\epsilon}} g_{t, i}
$$
其中$θ_{t+1,i}$ 表示 t+1 时刻的参数向量$θ_{t+1}$ 的第i个参数，$g_{k,i}$ 表示k时刻的梯度向量$g_k$ 的第i个维度（方向）。另外，**分母中求和的形式实现了退火过程**，这是很多优化技术中常见的策略，意味着随着时间推移，学习速率$\frac{\eta}{\sqrt{\sum_{k=0}^{t} g_{k, i}^{2}+\epsilon}}$越来越小，从而证了算法的最终收敛。

#### Adam方法

Adam方法将惯性保持和环境感知这两个优点集于一身。

- 一方面，Adam记录**梯度的一阶矩（first moment），即过往梯度与当前梯度的平均，这体现了惯性保持；**
- 另一方面，Adam还记录**梯度的二阶矩（second moment），即过往梯度平方与当前梯度平方的平均**，这类似AdaGrad方法，体现了环境感知能力，**为不同参数产生自适应的学习速率**

一阶矩和二阶矩采用类似于滑动窗口内求平均的思想进行融合，**即当前梯度和近一段时间内梯度的平均值，时间久远的梯度对当前平均值的贡献呈指数衰减**

具体来说，一阶矩和二阶矩采用指数衰退平均（exponential decay average）技术，计算公式为
$$
\begin{array}{l}m_{t}=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\ v_{t}=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}\end{array}
$$
其中$\beta_1, \beta_2$为衰减系数，$m_t$是一阶矩，$v_t$ 是二阶矩

如何理解一阶矩和二阶矩呢？

- 一阶矩相当于估计$E[g_t]$，由于当下梯度$g_t$是随机采样得到的估计结果，因此更关注它在统计意义上的**期望**

- 二阶矩相当于估计$E[{g_t}^2]$，这点与AdaGrad方法不同，不是${g_t}^2$ 从开始到现在的加和，而是它的**期望**

它们的**物理意义**是

- 当$||m_t||$大且 $v_t$ 大时，梯度大且稳定，这表明遇到一个明显的大坡，前进方向明确；

- 当$||m_t||$趋于零且$v_t$大时，梯度不稳定，表明可能遇到一个峡谷，容易引起反弹震荡；

- 当$||m_t||$大且$v_t$ 趋于零时，这种情况不可能出现；

- 当$||m_t||$趋 于零且$v_t$趋于零时，梯度趋于零，可能到达局部最低点，也可能走到一片坡度极缓的平地，此时要避免陷入平原

具体来说，**Adam的更新公式为:**
$$
\theta_{t+1}=\theta_{t}-\frac{\eta \cdot \hat{m}_{t}}{\sqrt{\hat{v}_{t}+\epsilon}}
$$
其中，$\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}, \quad \hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}$

