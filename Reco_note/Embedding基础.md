### 前言

- 本文记录了《深度学习推荐系统实战》第06讲Embedding基础的要点
- 课程地址：[深度学习推荐系统实战](https://time.geekbang.org/column/intro/349)

### 1. 什么是 Embedding

- 简单来说，**Embedding 就是用一个数值向量“表示”一个对象（Object）的方法**，这里说的对象可以是一个词、一个物品，也可以是一部电影等等。但是“表示”这个词是什么意思呢？用一个向量表示一个物品，这句话感觉还是有点让人费解。

- **一个物品能被向量表示，是因为这个向量跟其他物品向量之间的距离反映了这些物品的相似性**。更进一步来说，两个向量间的距离向量甚至能够反映它们之间的关系。这个解释听上去可能还是有点抽象，用两个具体的例子解释一下。

- 图 1 是 Google 著名的论文 Word2vec 中的例子，它利用 Word2vec 这个模型把单词映射到了高维空间中，每个单词在这个高维空间中的位置都非常有意思，你看图 1 左边的例子，从 king 到 queen 的向量和从 man 到 woman 的向量，无论从方向还是尺度来说它们都异常接近。这说明词 Embedding 向量间的运算居然能够揭示词之间的性别关系！比如 woman 这个词的词向量可以用下面的运算得出：

  `Embedding(woman)=Embedding(man)+[Embedding(queen)-Embedding(king)]`

- 同样，图 1 右的例子也很典型，从 walking 到 walked 和从 swimming 到 swam 的向量基本一致，这说明词向量揭示了词之间的时态关系！

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/18-1.jpg" style="zoom: 50%;" />

- Netflix 应用的电影 Embedding 向量方法，就是一个非常直接的推荐系统应用。从 **Netflix 利用矩阵分解方法生成的电影和用户的 Embedding 向量**示意图中，我们可以看出**不同的电影和用户分布在一个二维的空间内，由于 Embedding 向量保存了它们之间的相似性关系，因此有了这个 Embedding 空间之后，我们再进行电影推荐就非常容易了**。具体来说就是，我们直接找出某个用户向量周围的电影向量，然后把这些电影推荐给这个用户就可以了。这就是 Embedding 技术在推荐系统中最直接的应用。

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/18-2.jpg" style="zoom:50%;" />

### 2. Embedding 技术对深度学习推荐系统的重要性

- 首先，**Embedding 是处理稀疏特征的利器**。 因为推荐场景中的类别、ID 型特征非常多，大量使用 One-hot 编码会导致样本特征向量极度稀疏，而深度学习的结构特点又不利于稀疏特征向量的处理，因此几乎所有深度学习推荐模型都会由 **Embedding 层负责将稀疏高维特征向量转换成稠密低维特征向量**。所以说各类 Embedding 技术是构建深度学习推荐模型的基础性操作。
- **Embedding 可以融合大量有价值信息，本身就是极其重要的特征向量**。 相比由原始信息直接处理得来的特征向量，Embedding 的表达能力更强，特别是 Graph Embedding 技术被提出后，Embedding 几乎可以引入任何信息进行编码，使其本身就包含大量有价值的信息，所以**通过预训练得到的 Embedding 向量本身就是极其重要的特征向量。**
-  **Embedding不仅是一种处理稀疏特征的方法，也是融合大量基本特征，生成高阶特征向量的有效手段。**

### 3. 经典的 Embedding 方法：Word2vec

- Word2vec不仅让词向量在自然语言处理领域再度流行，更关键的是，自从 2013 年谷歌提出 Word2vec 以来，Embedding 技术从自然语言处理领域推广到广告、搜索、图像、推荐等几乎所有深度学习的领域，成了深度学习知识框架中不可或缺的技术点。

#### 3.1 什么是 Word2vec

- Word2vec 是“word to vector”的简称，顾名思义，它是一个生成对“词”的向量表达的模型。**想要训练 Word2vec 模型，我们需要准备由一组句子组成的语料库。假设其中一个长度为 T 的句子包含的词有 w1,w2……wt，并且我们假定每个词都跟其相邻词的关系最密切。**
- 根据模型假设的不同，Word2vec 模型分为两种形式，CBOW 模型（图 3 左）和 Skip-gram 模型（图 3 右）。其中，**CBOW 模型假设句子中每个词的选取都由相邻的词决定**，因此我们就看到 CBOW 模型的输入是 wt周边的词，预测的输出是 wt。S**kip-gram 模型则正好相反，它假设句子中的每个词都决定了相邻词的选取**，所以你可以看到 Skip-gram 模型的输入是 wt，预测的输出是 wt周边的词。按照一般的经验，Skip-gram 模型的效果会更好一些。

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/18-3.jpg" style="zoom:67%;" />

#### 3.2. Word2vec 的样本生成

- **训练 Word2vec 的样本是怎么生成**的。 作为一个自然语言处理的模型，**训练 Word2vec 的样本当然来自于语料库**，比如我们想训练一个电商网站中关键词的 Embedding 模型，那么电商网站中所有物品的描述文字就是很好的语料库。

- 我们**从语料库中抽取一个句子，选取一个长度为 2c+1（目标词前后各选 c 个词）的滑动窗口，将滑动窗口由左至右滑动，每移动一次，窗口中的词组就形成了一个训练样本**。根据 Skip-gram 模型的理念，中心词决定了它的相邻词，我们就可以根据这个训练样本定义出 Word2vec 模型的输入和输出，输入是样本的中心词，输出是所有的相邻词。
- 举一个例子。这里我们选取了“Embedding 技术对深度学习推荐系统的重要性”作为句子样本。**首先，我们对它进行分词、去除停用词的过程，生成词序列，再选取大小为 3 的滑动窗口从头到尾依次滑动生成训练样本，然后我们把中心词当输入，边缘词做输出，就得到了训练 Word2vec 模型可用的训练样本**。

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/18-4.jpg" style="zoom:67%;" />

#### 3.3. Word2vec 模型的结构

- Word2vec结构本质上就是一个三层的神经网络:

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/18-5.jpg" style="zoom:50%;" />

- **它的输入层和输出层的维度都是 V，这个 V 其实就是语料库词典的大小**。假设语料库一共使用了 10000 个词，那么 V 就等于 10000。根据图 4 生成的训练样本，这里的**输入向量自然就是由输入词转换而来的 One-hot 编码向量，输出向量则是由多个输出词转换而来的 Multi-hot 编码向量**，显然，基于 Skip-gram 框架的 Word2vec 模型解决的是一个多分类问题。
- 隐层的维度是 N，N 的选择就需要一定的调参能力了，我们需要对模型的效果和模型的复杂度进行权衡，来决定最后 N 的取值，并且最终每个词的 Embedding 向量维度也由 N 来决定。最后是激活函数的问题，这里我们需要注意的是，**隐层神经元是没有激活函数的，或者说采用了输入即输出的恒等函数作为激活函数，而输出层神经元采用了 softmax 作为激活函数**。

- 为什么要这样设置 Word2vec 的神经网络，以及我们为什么要这样选择激活函数呢？因为这个神经网络其实是为了表达从输入向量到输出向量的这样的一个条件概率关系，看下面的式子

$$
p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\prime} v_{w_{I}}\right)}{\sum_{i=1}^{V} \exp \left(v_{w_{i}}^{\prime}{ }^{\top} v_{w_{I}}\right)}
$$

- 这个**由输入词 WI 预测输出词 WO 的条件概率，其实就是 Word2vec 神经网络要表达的东西**。我们**通过极大似然的方法去最大化这个条件概率，就能够让相似的词的内积距离更接近**，这就是我们希望 Word2vec 神经网络学到的。

- Word2vec 还有很多值得挖掘的东西，比如，**为了节约训练时间，Word2vec 经常会采用负采样（Negative Sampling）或者分层 softmax（Hierarchical Softmax）的训练方法**。

#### 3.4. 怎样把词向量从 Word2vec 模型中提取出来

- 在**训练完 Word2vec 的神经网络之后，我们不是想得到每个词对应的 Embedding 向量嘛，这个 Embedding 在哪呢？其实，它就藏在输入层到隐层的权重矩阵 WVxN 中**。看了下面的图就明白了

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/18-6.jpg" style="zoom:67%;" />

- **输入向量矩阵 WVxN 的每一个行向量对应的就是我们要找的“词向量”**。比如我们要找**词典里第 i 个词对应的 Embedding，因为输入向量是采用 One-hot 编码的，所以输入向量的第 i 维就应该是 1，那么输入向量矩阵 WVxN 中第 i 行的行向量自然就是该词的 Embedding** 。输出向量矩阵 W′ 也遵循这个道理，确实是这样的，但一般来说，我们还是**习惯于使用输入向量矩阵作为词向量矩阵**。

- 在实际的使用过程中，我们往往**会把输入向量矩阵转换成词向量查找表**（Lookup table，如图 7 所示）。例如，**输入向量是 10000 个词组成的 One-hot 向量，隐层维度是 300 维，那么输入层到隐层的权重矩阵为 10000x300 维。在转换为词向量 Lookup table 后，每行的权重即成了对应词的 Embedding 向量**。如果我们把这个查找表存储到线上的数据库中，就可以轻松地在推荐物品的过程中使用 Embedding 去计算相似性等重要的特征了。

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/18-7.jpg" style="zoom:50%;" />

### 4. Item2Vec：Word2vec 方法的推广

-  Word2vec 可以对词“序列”中的词进行 Embedding，那么对于**用户购买“序列”中的一个商品，用户观看“序列”中的一个电影，也应该存在相应的 Embedding 方法。**

<img src="https://blog-1258986886.cos.ap-beijing.myqcloud.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/18-8.jpg" style="zoom:67%;" />

- 微软 2015 年提出 Item2Vec ，它是对 Word2vec 方法的推广，**使 Embedding 方法适用于几乎所有的序列数据。Item2Vec 模型的技术细节几乎和 Word2vec 完全一致，只要能够用序列数据的形式把我们要表达的对象表示出来，再把序列数据“喂”给 Word2vec 模型，我们就能够得到任意物品的 Embedding** 了。

- tem2vec 的提出对于推荐系统来说当然是至关重要的，因为它使得“万物皆 Embedding”成为了可能。**对于推荐系统来说，Item2vec 可以利用物品的 Embedding 直接求得它们的相似性，或者作为重要的特征输入推荐模型进行训练，这些都有助于提升推荐系统的效果。**

